{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, roc_auc_score,matthews_corrcoef, precision_recall_curve\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop with settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngram(text_representation):\n",
    "    #transform string text representation to suitable format for sklearn vectorizers\n",
    "    if text_representation=='UNIGRAM':\n",
    "        ngram_range=(1,1)\n",
    "    elif text_representation=='BIGRAM':\n",
    "        ngram_range=(1,2)\n",
    "    elif text_representation=='TRIGRAM':\n",
    "        ngram_range=(1,3)\n",
    "    return ngram_range\n",
    "\n",
    "def perform_tf_idf(X_train,X_test,ngram_range,analyzer,use_idf):\n",
    "    #perform tfidf vectorization \n",
    "    \n",
    "    #initialize values and tfidf-vectorizer\n",
    "    min_df,max_df,max_features = 10, 1., 300\n",
    "    tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                            analyzer=analyzer,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True,\n",
    "                           use_idf=use_idf)\n",
    "    \n",
    "    #transform text data to tfidf representation\n",
    "    features_train = tfidf.fit_transform(X_train).toarray()\n",
    "    features_test = tfidf.transform(X_test).toarray()\n",
    "    return features_train,features_test\n",
    "\n",
    "def perform_countvectorizer(X_train,X_test,ngram_range,analyzer):\n",
    "    #initialize values for countvectorizer\n",
    "    min_df,max_df,max_features = 10, 1., 300\n",
    "   \n",
    "    countvect = CountVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,analyzer=analyzer,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features)\n",
    "    \n",
    "    #transform text data to binary features\n",
    "    features_train = countvect.fit_transform(X_train).toarray()\n",
    "    features_test = countvect.transform(X_test).toarray()\n",
    "    return features_train,features_test    \n",
    "\n",
    "def perform_info_gain_df(X_train,X_test,labels,ngram_range,analyzer):\n",
    "    from info_gain import info_gain\n",
    "    \n",
    "    #initialize values and countvectorizer\n",
    "    X_train=X_train.astype(str)\n",
    "    X_test=X_test.astype(str)\n",
    "    min_df,max_df,max_features = 10, 1., 300\n",
    "    countvect = CountVectorizer(encoding='utf-8',ngram_range=ngram_range,analyzer=analyzer,stop_words=None,\n",
    "                                lowercase=False,max_df=max_df,min_df=min_df,max_features=max_features,binary=True)\n",
    "  \n",
    "    #transform text data to binary representation\n",
    "    dftrain = pd.DataFrame(countvect.fit_transform(X_train).toarray(), columns=countvect.get_feature_names())\n",
    "    dftest = pd.DataFrame(countvect.transform(X_test).toarray(), columns=countvect.get_feature_names())\n",
    "    \n",
    "    dftrain['RESPONSIVE']=labels\n",
    "    \n",
    "    #initialize dictonary with info gain per feature\n",
    "    ig_dict={}\n",
    "    for column in dftrain.columns:\n",
    "        ig  = info_gain.info_gain(dftrain['RESPONSIVE'], dftrain[column])\n",
    "        ig_dict[column]=ig\n",
    "    \n",
    "    for column in dftrain.columns:\n",
    "        info_gain=ig_dict[column]\n",
    "        dftrain[column]=np.where(dftrain[column] == 0, 0, info_gain)\n",
    "    for column in dftest.columns:\n",
    "        info_gain=ig_dict[column]\n",
    "        dftest[column]=np.where(dftest[column] == 0, 0, info_gain)\n",
    "    return dftrain.loc[:, dftrain.columns != 'RESPONSIVE'],dftest\n",
    "\n",
    "    \n",
    "def train_classifier(data,model,iterations,cv,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "    #this function does al preprocessing steps and trains a given classifier in a standardized way\n",
    "    \n",
    "    #split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['TOKENS'],   #ALLEEN VOOR ENRON\n",
    "                                                    data['RESPONSIVE'], \n",
    "                                                    test_size=0.15)\n",
    "    X_train=X_train.astype(str)\n",
    "    X_test=X_test.astype(str)    #weet niet zeker of dit moet\n",
    "    \n",
    "    #extract n-grams from the text\n",
    "    ngram_range = extract_ngram(text_representation)\n",
    "    \n",
    "    # perform a text representation method on the data\n",
    "    if feature_weighting =='TF-IDF':\n",
    "        features_train,features_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),True)\n",
    "    elif feature_weighting =='TF':\n",
    "        features_train,features_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),False)\n",
    "    elif feature_weighting=='BINARY':\n",
    "        features_train,features_test=perform_countvectorizer(X_train,X_test,ngram_range,wordchar.lower())\n",
    "    elif feature_weighting=='INFO-GAIN':\n",
    "        features_train,features_test=perform_info_gain_df(X_train,X_test,y_train,ngram_range,wordchar.lower())\n",
    "    \n",
    "    labels_train = y_train\n",
    "    labels_test = y_test\n",
    "    \n",
    "    #determine usage of SMOTE\n",
    "    if smoteennyes:\n",
    "        features_train, labels_train = SMOTEENN().fit_resample(features_train, labels_train)\n",
    "        \n",
    "    #determine usage of undersampling    \n",
    "    if underyes:\n",
    "        features_train, labels_train =RandomUnderSampler(sampling_strategy=1).fit_resample(features_train,labels_train)\n",
    "    \n",
    "    #determine usage of class weights\n",
    "    if classweightyes:\n",
    "        class_weight='balanced'\n",
    "    else:\n",
    "        class_weight=None\n",
    "    \n",
    "    #initialize estimator\n",
    "    if model=='svm':\n",
    "        estimator= SVC(random_state=8)\n",
    "        random_grid=svm_grid\n",
    "    elif model=='rf':\n",
    "        estimator= RandomForestClassifier(random_state=8)\n",
    "        random_grid=rf_grid\n",
    "    elif model=='cnb':\n",
    "        estimator= ComplementNB()\n",
    "        random_grid=cnb_grid\n",
    "    elif model=='xgb':\n",
    "        estimator=XGBClassifier(random_state=8)\n",
    "        ratio_w=len(data[data['RESPONSIVE']==0])/len(data[data['RESPONSIVE']==1])\n",
    "        random_grid=xgb_grid\n",
    "    \n",
    "    # init the random search algorithm\n",
    "    random_search = RandomizedSearchCV(estimator=estimator,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=iterations,\n",
    "                                   scoring=ftwo_scorer,\n",
    "                                   cv=cv, \n",
    "                                   verbose=1, \n",
    "                                   refit=True)\n",
    "    \n",
    "    #check for usage of sample weights and fit random search algorithm\n",
    "    if sampleweightyes:\n",
    "        sample_weight = compute_sample_weight(class_weight='balanced', y=labels_train)\n",
    "        random_search.fit(features_train, labels_train,sample_weight=sample_weight)\n",
    "    else:\n",
    "        random_search.fit(features_train, labels_train)\n",
    "   \n",
    "    y_pred=random_search.predict(features_test)\n",
    "    y_probas=random_search.predict_proba(features_test)\n",
    "    \n",
    "    f2_score_before_thresholding=fbeta_score(labels_test,y_pred,beta=2)\n",
    "    p, r, thresholds = precision_recall_curve(y_test, y_probas[:,1])\n",
    "\n",
    "    f2_list=[]\n",
    "    \n",
    "    #calculate f2 scores for each threshold\n",
    "    for i in range(0,len(p)):\n",
    "        f2_list.append((5*p[i]*r[i])/((4*p[i])+r[i]))\n",
    "    \n",
    "    #find optimal threshold\n",
    "    t=thresholds[f2_list.index(max(f2_list))]\n",
    "    \n",
    "    #calculate new labels\n",
    "    new_labels=adjusted_classes(y_probas[:,1],t)\n",
    "    \n",
    "    #calculate f2 score after thresholding\n",
    "    f2_score=fbeta_score(y_test,new_labels,beta=2)\n",
    "    \n",
    "    return f1_score(labels_test,y_pred),f2_score,f2_score_before_thresholding,random_search.best_params_\n",
    "\n",
    "\n",
    "# init variable values and create parameter grid for estimators\n",
    "class_weight='balanced'\n",
    "ratio_w=1\n",
    "\n",
    "svm_grid = {'C': [.01,.1,.4,.6,1,1.5],\n",
    "              'kernel': [ 'rbf', 'poly','linear'],\n",
    "              'gamma':  [.01, .1, 1],\n",
    "              'degree': [1, 2, 3, 4, 5],\n",
    "              'probability': [True],\n",
    "              'class_weight':[class_weight]\n",
    "             }\n",
    "\n",
    "xgb_grid = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'scale_pos_weight':[ratio_w] \n",
    "}\n",
    "\n",
    "rf_grid={'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False],\n",
    "                'class_weight':['balanced']}\n",
    "\n",
    "cnb_grid={'alpha':[0,0.3,0.6,0.8,1], \n",
    "          'fit_prior':[False,True],\n",
    "          'norm':[False,True]}\n",
    "\n",
    "\n",
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]\n",
    "\n",
    "\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_testing(data,model,best_params,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "    # testing function that does unique train/test split and applies already found parameters for validation\n",
    "    \n",
    "    #split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['TOKENS'], \n",
    "                                                    data['RESPONSIVE'], \n",
    "                                                    test_size=0.15,stratify=data['RESPONSIVE'])\n",
    "                                                    \n",
    "    #split train/val\n",
    "    X_train_1, X_val, y_train_1, y_val=train_test_split(X_train,  \n",
    "                                                    y_train, \n",
    "                                                    test_size=0.15,stratify=y_train)               \n",
    "    \n",
    "    \n",
    "    def fit_and_val_model(model,best_params,X_train_1,y_train_1,X_val,y_val,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "        #fit and validation function\n",
    "        \n",
    "        \n",
    "        X_train_1,X_val=X_train_1.astype(str),X_val.astype(str) \n",
    "        \n",
    "        #extract ngram representation\n",
    "        ngram_range = extract_ngram(text_representation)\n",
    "        \n",
    "        #perform vectorization of text data\n",
    "        if feature_weighting =='TF-IDF':\n",
    "            X_train_1,X_val=perform_tf_idf(X_train_1,X_val,ngram_range,wordchar.lower(),True)\n",
    "        elif feature_weighting =='TF':\n",
    "            X_train_1,X_val=perform_tf_idf(X_train_1,X_val,ngram_range,wordchar.lower(),False)\n",
    "        elif feature_weighting=='BINARY':\n",
    "            X_train_1,X_val=perform_countvectorizer(X_train_1,X_val,ngram_range,wordchar.lower())\n",
    "        elif feature_weighting=='INFO-GAIN':\n",
    "            X_train_1,X_val=perform_info_gain_df(X_train_1,X_val,y_train_1,ngram_range,wordchar.lower())\n",
    "            \n",
    "        y_train_1,y_val = y_train_1,y_val\n",
    "        \n",
    "        #check whether SMOTE has to be performed\n",
    "        if smoteennyes:\n",
    "            X_train_1, y_train_1 = SMOTEENN().fit_resample(X_train_1, y_train_1)\n",
    "        \n",
    "        #check whether undersampling has to be performed\n",
    "        if underyes:\n",
    "            X_train_1,y_train_1 =RandomUnderSampler(sampling_strategy=1).fit_resample(X_train_1,y_train_1)\n",
    "        \n",
    "        #check whether classweights have to be applied\n",
    "        if classweightyes:\n",
    "            class_weight='balanced'\n",
    "        else:\n",
    "            class_weight=None\n",
    "        \n",
    "        #check which estimator will be used and use optimal parameters\n",
    "        if model=='svm':\n",
    "            estimator= SVC(C=best_params['C'],kernel=best_params['kernel'],gamma=best_params['gamma'],degree=best_params['degree'],probability=best_params['probability'],class_weight=class_weight)\n",
    "        elif model=='rf':\n",
    "            estimator= RandomForestClassifier(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth'],min_samples_split=best_params['min_samples_split'],min_samples_leaf=best_params['min_samples_leaf'],bootstrap=best_params['bootstrap'],class_weight='balanced')\n",
    "        elif model=='cnb':\n",
    "            estimator= ComplementNB(alpha=best_params['alpha'],fit_prior=best_params['fit_prior'],norm=best_params['norm'])\n",
    "        elif model=='xgb':\n",
    "            ratio_w=y_train_1.value_counts()[0]/y_train_1.value_counts()[1]\n",
    "            estimator=XGBClassifier(min_child_weight=best_params['min_child_weight'],gamma=best_params['gamma'],subsample=best_params['subsample'],colsample_bytree=best_params['colsample_bytree'],max_depth=best_params['max_depth'], scale_pos_weight=ratio_w)\n",
    "         \n",
    "        #check whether sample weights have to be applied and fit estimator\n",
    "        if sampleweightyes:\n",
    "            sample_weight = compute_sample_weight(class_weight='balanced', y=y_train_1)\n",
    "            estimator.fit(X_train_1, y_train_1,sample_weight=sample_weight)\n",
    "        else:\n",
    "            estimator.fit(X_train_1,y_train_1)\n",
    "        \n",
    "        #predict labels\n",
    "        y_pred=estimator.predict(X_val)\n",
    "        \n",
    "        #predict probabilities\n",
    "        y_probas=estimator.predict_proba(X_val)\n",
    "        \n",
    "        #calculate f2 score before thresholding\n",
    "        f2_score_before_thresholding=fbeta_score(y_val,y_pred,beta=2)\n",
    "        \n",
    "        #calculate optimal threshold\n",
    "        p, r, thresholds = precision_recall_curve(y_val, y_probas[:,1])\n",
    "        f2_list=[]\n",
    "        for i in range(0,len(p)):\n",
    "            f2_list.append((5*p[i]*r[i])/((4*p[i])+r[i]))\n",
    "        t=thresholds[f2_list.index(max(f2_list))]\n",
    "        \n",
    "        #calculate new labels and optimal f2 score\n",
    "        new_labels=adjusted_classes(y_probas[:,1],t)\n",
    "        f2_score=fbeta_score(y_val,new_labels,beta=2)\n",
    "        return t\n",
    "\n",
    "    def test_model(model,best_params,X_train,y_train,X_test,y_test,t,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "        #test model for final validation\n",
    "        \n",
    "        \n",
    "        X_train,X_test=X_train.astype(str),X_test.astype(str) \n",
    "        \n",
    "        #extract ngram representation\n",
    "        ngram_range = extract_ngram(text_representation)\n",
    "        \n",
    "        #perform vectorization of text data\n",
    "        if feature_weighting =='TF-IDF':\n",
    "            X_train,X_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),True)\n",
    "        elif feature_weighting =='TF':\n",
    "            X_train,X_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),False)\n",
    "        elif feature_weighting=='BINARY':\n",
    "            X_train,X_test=perform_countvectorizer(X_train,X_test,ngram_range,wordchar.lower())\n",
    "        elif feature_weighting=='INFO-GAIN':\n",
    "            X_train,X_test=perform_info_gain_df(X_train,X_test,y_train,ngram_range,wordchar.lower())   \n",
    "        \n",
    "        #check whether SMOTE has to be performed\n",
    "        if smoteennyes:\n",
    "            X_train, y_train = SMOTEENN().fit_resample(X_train, y_train)\n",
    "        \n",
    "        #check whether undersampling has to be performed\n",
    "        if underyes:\n",
    "            X_train,y_train =RandomUnderSampler(sampling_strategy=1).fit_resample(X_train,y_train)\n",
    "        \n",
    "        #check whether classweights have to be applied\n",
    "        if classweightyes:\n",
    "            class_weight='balanced'\n",
    "        else:\n",
    "            class_weight=None\n",
    "       \n",
    "        #check which estimator will be used and use optimal parameters\n",
    "        if model=='svm':\n",
    "            estimator= SVC(C=best_params['C'],kernel=best_params['kernel'],gamma=best_params['gamma'],degree=best_params['degree'],probability=best_params['probability'],class_weight=class_weight)\n",
    "        elif model=='rf':\n",
    "            estimator= RandomForestClassifier(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth'],min_samples_split=best_params['min_samples_split'],min_samples_leaf=best_params['min_samples_leaf'],bootstrap=best_params['bootstrap'],class_weight='balanced')\n",
    "        elif model=='cnb':\n",
    "            estimator= ComplementNB(alpha=best_params['alpha'],fit_prior=best_params['fit_prior'],norm=best_params['norm'])\n",
    "        elif model=='xgb':\n",
    "            ratio_w=y_train.value_counts()[0]/y_train.value_counts()[1]\n",
    "            estimator=XGBClassifier(min_child_weight=best_params['min_child_weight'],gamma=best_params['gamma'],subsample=best_params['subsample'],colsample_bytree=best_params['colsample_bytree'],max_depth=best_params['max_depth'], scale_pos_weight=ratio_w)\n",
    "         \n",
    "        #check whether sample weights have to be applied and fit estimator\n",
    "        if sampleweightyes:\n",
    "            sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "            estimator.fit(X_train, y_train,sample_weight=sample_weight)\n",
    "        else:\n",
    "            estimator.fit(X_train,y_train)\n",
    "        \n",
    "        #predict labels\n",
    "        y_pred=estimator.predict(X_test)\n",
    "        \n",
    "        #predict probabilities\n",
    "        y_probas=estimator.predict_proba(X_test)\n",
    "        \n",
    "        #calculate f2 score before thresholding\n",
    "        f2_score_without_thresholding=fbeta_score(y_test,y_pred,beta=2)\n",
    "        \n",
    "        #calculate new labels and optimal f2 score\n",
    "        new_labels=adjusted_classes(y_probas[:,1],t)\n",
    "        f2_score=fbeta_score(y_test,new_labels,beta=2)\n",
    "        \n",
    "        #calculate roc-auc score,precision,fpr,fnr and recall\n",
    "        roc_score=roc_auc_score(y_test,y_probas[:,1])\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test,new_labels).ravel()\n",
    "        precision=tp/(tp+fp)\n",
    "        fpr = fp/(fp+tn)\n",
    "        fnr=fn/(fn+tp)\n",
    "        recall=tp/(tp+fn)\n",
    "        \n",
    "        return f2_score, f2_score_without_thresholding,roc_score,precision,fpr,fnr,recall\n",
    "    \n",
    "    #fit and validate model and use optimal threshold of training data on test data\n",
    "    t=fit_and_val_model(model,best_params,X_train_1,y_train_1,X_val,y_val,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting)\n",
    "    return test_model(model,best_params,X_train,y_train,X_test,y_test,t,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting)\n",
    "\n",
    "\n",
    "\n",
    "def testing_statistics(data,model,best_params,iterations,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):  #oneven aantal!!!\n",
    "    #this function validates the model x iterations and saves all scores\n",
    "    \n",
    "    f2_thresh_scores=[]\n",
    "    f2_no_thresh_scores=[]\n",
    "    roc_scores=[]\n",
    "    precision_scores=[]\n",
    "    fpr_scores=[]\n",
    "    fnr_scores=[]\n",
    "    recall_scores=[]\n",
    "    \n",
    "    for iter in range(0,iterations):\n",
    "        f2_thresh,f2_no_thresh,roc_score,precision,fpr,fnr,recall=do_testing(data,model,best_params,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting)\n",
    "        f2_thresh_scores.append(f2_thresh)\n",
    "        f2_no_thresh_scores.append(f2_no_thresh)\n",
    "        roc_scores.append(roc_score)\n",
    "        precision_scores.append(precision)\n",
    "        fpr_scores.append(fpr)\n",
    "        fnr_scores.append(fnr)\n",
    "        recall_scores.append(recall)\n",
    "    return f2_thresh_scores, f2_no_thresh_scores,roc_scores,precision_scores,fpr_scores,fnr_scores,recall_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
