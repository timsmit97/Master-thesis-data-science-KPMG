{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, roc_curve,roc_auc_score,matthews_corrcoef, precision_recall_curve\n",
    "from sklearn.metrics import fbeta_score, make_scorer,accuracy_score\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import average_precision_score\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in optimal settings per text classifier per dataset\n",
    "top_models=pd.DataFrame(columns=['Dataset','Text representation','Word/char','Weighting','Algorithm','Smoteenn','Undersampling','Classweights','Sampleweights'])\n",
    "top_models.loc[0]=['WHISKEY','BIGRAM','WORD','TF','rf','TRUE','TRUE','FALSE','TRUE']\n",
    "top_models.loc[1]=['2020','TRIGRAM','WORD','TF-IDF','svm','TRUE','TRUE','TRUE','TRUE']\n",
    "top_models.loc[2]=['ENRON','UNIGRAM','WORD','TF','rf','FALSE','TRUE','TRUE','FALSE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_domain_report(boolean,transfername,t,random_search,y_pred,X_target,y_target,labels_test,visualisatie_controle,balanced_learning):\n",
    "    #this function reports the performance metrics of the model\n",
    "    \n",
    "    f2_score_before_thresholding=fbeta_score(labels_test,y_pred,beta=2)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_test,y_pred).ravel()\n",
    "    fnr=fn/(fn+tp)\n",
    "    fpr=fp/(fp+tn)\n",
    "\n",
    "    #predict probabilities and obtain new labels with optimal threshold\n",
    "    probas=random_search.predict_proba(X_target)[:,1]\n",
    "    new_labels=adjusted_classes(probas,t)\n",
    "   \n",
    "    f2_score=fbeta_score(y_target,new_labels,beta=2)\n",
    "    acc=accuracy_score(y_target,new_labels)\n",
    "    roc_auc=roc_auc_score(y_target,probas)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_test,new_labels).ravel()\n",
    "    fpr_after_thresh=fp/(fp+tn)\n",
    "    fnr_after_thresh=fn/(fn+tp)\n",
    "    \n",
    "    return roc_auc,f2_score_before_thresholding,fpr,fnr,f2_score,fpr_after_thresh,fnr_after_thresh,acc\n",
    "\n",
    "def do_cross_domains(transfername,dfsource,dftarget,algorithm,Smoteenn,Undersampling,Classweights,Sampleweights,Text_representation,Wordchar,Weighting,visualisatie_controle,balanced_learning):\n",
    "    \n",
    "    #train classifier to predict from domain A to domain B\n",
    "    random_search,t,X_target,best_params=train_classifier(dfsource,dftarget,algorithm,10,2,Smoteenn,Undersampling,Classweights,Sampleweights,Text_representation,Wordchar,Weighting)\n",
    "    \n",
    "    #obtain performance metrics of the predictions\n",
    "    roc_auc,f2,fpr,fnr,f2_after_thresh,fpr_after_thresh,fnr_after_thresh,acc=cross_domain_report(boolean,transfername,t,random_search,random_search.predict(X_target),X_target,dftarget['RESPONSIVE'],dftarget['RESPONSIVE'],visualisatie_controle,balanced_learning)\n",
    "    \n",
    "    return roc_auc,f2,fpr,fnr,f2_after_thresh,fpr_after_thresh,fnr_after_thresh,acc\n",
    "\n",
    "def apply_best_settings_cross_domain(top_models,dfwhiskey,df2020,visualisatie_controle,balanced_learning):\n",
    "    #This function does all cross-domain predictions and uses optimal source domain parameters \n",
    "    \n",
    "    #init score dataframe\n",
    "    plain_baseline=pd.DataFrame(columns=['Source','Target','F2-score','ROC-AUC','FPR','FNR','tussenkolom','F2 on 10% target thresholding','FPR after thresholding','FNR after thresholding','Accuracy'])\n",
    "    \n",
    "    #obtain opt parameters\n",
    "    row_w=top_models.loc[0]\n",
    "    row_2020=top_models.loc[1]\n",
    "    \n",
    "    #apply 1:1 sampling or not \n",
    "    if balanced_learning:\n",
    "        dfwhiskey,df2020=create_balanced_sets(dfwhiskey,dfwhiskey['RESPONSIVE'],df2020,df2020['RESPONSIVE'])\n",
    "        \n",
    "    #validate both 100 times\n",
    "    for count in range(0,100):\n",
    "        #train and predicts across domains\n",
    "        roc_auc,f2,fpr,fnr,f2_after_thresh,fpr_after_thresh,fnr_after_thresh,acc,boolean=do_cross_domains(boolean,'whiskey2020',dfwhiskey,df2020,row_w['Algorithm'],row_w['Smoteenn'],row_w['Undersampling'],row_w['Classweights'],row_w['Sampleweights'],row_w['Text representation'],row_w['Word/char'],row_w['Weighting'],visualisatie_controle,balanced_learning)\n",
    "        \n",
    "        #update score dataframe\n",
    "        plain_baseline.loc[count]=['whiskey','2020',f2,roc_auc,fpr,fnr,'-',f2_after_thresh,fpr_after_thresh,fnr_after_thresh,acc]\n",
    "        \n",
    "        #write scores to directory every iteration\n",
    "        plain_baseline.to_excel(\"Plain ACCURACY baseline 1;1 gesampled.xlsx\")\n",
    "\n",
    "    for count in range(100,200):\n",
    "        #train and predicts across domains\n",
    "        roc_auc,f2,fpr,fnr,f2_after_thresh,fpr_after_thresh,fnr_after_thresh,acc,boolean=do_cross_domains(boolean,'2020whiskey',df2020,dfwhiskey,row_2020['Algorithm'],row_2020['Smoteenn'],row_2020['Undersampling'],row_2020['Classweights'],row_2020['Sampleweights'],row_2020['Text representation'],row_2020['Word/char'],row_2020['Weighting'],visualisatie_controle,balanced_learning)    \n",
    "        \n",
    "        #update score dataframe\n",
    "        plain_baseline.loc[count]=['2020','whiskey',f2,roc_auc,fpr,fnr,'-',f2_after_thresh,fpr_after_thresh,fnr_after_thresh,acc]\n",
    "        \n",
    "        #write scores to directory every iteration\n",
    "        plain_baseline.to_excel(\"Plain ACCURACY baseline 1;1 gesampled.xlsx\")\n",
    "\n",
    "    return plain_baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngram(text_representation):\n",
    "    #transform string text representation to suitable format for sklearn vectorizers\n",
    "    if text_representation=='UNIGRAM':\n",
    "        ngram_range=(1,1)\n",
    "    elif text_representation=='BIGRAM':\n",
    "        ngram_range=(1,2)\n",
    "    elif text_representation=='TRIGRAM':\n",
    "        ngram_range=(1,3)\n",
    "    return ngram_range\n",
    "\n",
    "def perform_tf_idf(X_train,X_test,ngram_range,analyzer,use_idf):\n",
    "    #perform tfidf vectorization \n",
    "    \n",
    "    #initialize values and tfidf-vectorizer\n",
    "    min_df,max_df,max_features = 10, 1., 300\n",
    "    tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                            analyzer=analyzer,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True,\n",
    "                           use_idf=use_idf)\n",
    "    \n",
    "    #transform text data to tfidf representation\n",
    "    features_train = tfidf.fit_transform(X_train).toarray()\n",
    "    features_test = tfidf.transform(X_test).toarray()\n",
    "    return features_train,features_test\n",
    "\n",
    "def perform_countvectorizer(X_train,X_test,ngram_range,analyzer):\n",
    "    #initialize values for countvectorizer\n",
    "    min_df,max_df,max_features = 10, 1., 300\n",
    "   \n",
    "    countvect = CountVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,analyzer=analyzer,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features)\n",
    "    \n",
    "    #transform text data to binary features\n",
    "    features_train = countvect.fit_transform(X_train).toarray()\n",
    "    features_test = countvect.transform(X_test).toarray()\n",
    "    return features_train,features_test    \n",
    "\n",
    "def perform_info_gain_df(X_train,X_test,labels,ngram_range,analyzer):\n",
    "    from info_gain import info_gain\n",
    "    \n",
    "    #initialize values and countvectorizer\n",
    "    X_train=X_train.astype(str)\n",
    "    X_test=X_test.astype(str)\n",
    "    min_df,max_df,max_features = 10, 1., 300\n",
    "    countvect = CountVectorizer(encoding='utf-8',ngram_range=ngram_range,analyzer=analyzer,stop_words=None,\n",
    "                                lowercase=False,max_df=max_df,min_df=min_df,max_features=max_features,binary=True)\n",
    "  \n",
    "    #transform text data to binary representation\n",
    "    dftrain = pd.DataFrame(countvect.fit_transform(X_train).toarray(), columns=countvect.get_feature_names())\n",
    "    dftest = pd.DataFrame(countvect.transform(X_test).toarray(), columns=countvect.get_feature_names())\n",
    "    \n",
    "    dftrain['RESPONSIVE']=labels\n",
    "    \n",
    "    #initialize dictonary with info gain per feature\n",
    "    ig_dict={}\n",
    "    for column in dftrain.columns:\n",
    "        ig  = info_gain.info_gain(dftrain['RESPONSIVE'], dftrain[column])\n",
    "        ig_dict[column]=ig\n",
    "    \n",
    "    for column in dftrain.columns:\n",
    "        info_gain=ig_dict[column]\n",
    "        dftrain[column]=np.where(dftrain[column] == 0, 0, info_gain)\n",
    "    for column in dftest.columns:\n",
    "        info_gain=ig_dict[column]\n",
    "        dftest[column]=np.where(dftest[column] == 0, 0, info_gain)\n",
    "    return dftrain.loc[:, dftrain.columns != 'RESPONSIVE'],dftest\n",
    "\n",
    "    \n",
    "def train_classifier(data,model,iterations,cv,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "    #this function does al preprocessing steps and trains a given classifier in a standardized way\n",
    "    \n",
    "    #split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['TOKENS'],   #ALLEEN VOOR ENRON\n",
    "                                                    data['RESPONSIVE'], \n",
    "                                                    test_size=0.15)\n",
    "    X_train=X_train.astype(str)\n",
    "    X_test=X_test.astype(str)    #weet niet zeker of dit moet\n",
    "    \n",
    "    #extract n-grams from the text\n",
    "    ngram_range = extract_ngram(text_representation)\n",
    "    \n",
    "    # perform a text representation method on the data\n",
    "    if feature_weighting =='TF-IDF':\n",
    "        features_train,features_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),True)\n",
    "    elif feature_weighting =='TF':\n",
    "        features_train,features_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),False)\n",
    "    elif feature_weighting=='BINARY':\n",
    "        features_train,features_test=perform_countvectorizer(X_train,X_test,ngram_range,wordchar.lower())\n",
    "    elif feature_weighting=='INFO-GAIN':\n",
    "        features_train,features_test=perform_info_gain_df(X_train,X_test,y_train,ngram_range,wordchar.lower())\n",
    "    \n",
    "    labels_train = y_train\n",
    "    labels_test = y_test\n",
    "    \n",
    "    #determine usage of SMOTE\n",
    "    if smoteennyes:\n",
    "        features_train, labels_train = SMOTEENN().fit_resample(features_train, labels_train)\n",
    "        \n",
    "    #determine usage of undersampling    \n",
    "    if underyes:\n",
    "        features_train, labels_train =RandomUnderSampler(sampling_strategy=1).fit_resample(features_train,labels_train)\n",
    "    \n",
    "    #determine usage of class weights\n",
    "    if classweightyes:\n",
    "        class_weight='balanced'\n",
    "    else:\n",
    "        class_weight=None\n",
    "    \n",
    "    #initialize estimator\n",
    "    if model=='svm':\n",
    "        estimator= SVC(random_state=8)\n",
    "        random_grid=svm_grid\n",
    "    elif model=='rf':\n",
    "        estimator= RandomForestClassifier(random_state=8)\n",
    "        random_grid=rf_grid\n",
    "    elif model=='cnb':\n",
    "        estimator= ComplementNB()\n",
    "        random_grid=cnb_grid\n",
    "    elif model=='xgb':\n",
    "        estimator=XGBClassifier(random_state=8)\n",
    "        ratio_w=len(data[data['RESPONSIVE']==0])/len(data[data['RESPONSIVE']==1])\n",
    "        random_grid=xgb_grid\n",
    "    \n",
    "    # init the random search algorithm\n",
    "    random_search = RandomizedSearchCV(estimator=estimator,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=iterations,\n",
    "                                   scoring=ftwo_scorer,\n",
    "                                   cv=cv, \n",
    "                                   verbose=1, \n",
    "                                   refit=True)\n",
    "    \n",
    "    #check for usage of sample weights and fit random search algorithm\n",
    "    if sampleweightyes:\n",
    "        sample_weight = compute_sample_weight(class_weight='balanced', y=labels_train)\n",
    "        random_search.fit(features_train, labels_train,sample_weight=sample_weight)\n",
    "    else:\n",
    "        random_search.fit(features_train, labels_train)\n",
    "   \n",
    "    y_pred=random_search.predict(features_test)\n",
    "    y_probas=random_search.predict_proba(features_test)\n",
    "    \n",
    "    f2_score_before_thresholding=fbeta_score(labels_test,y_pred,beta=2)\n",
    "    p, r, thresholds = precision_recall_curve(y_test, y_probas[:,1])\n",
    "\n",
    "    f2_list=[]\n",
    "    \n",
    "    #calculate f2 scores for each threshold\n",
    "    for i in range(0,len(p)):\n",
    "        f2_list.append((5*p[i]*r[i])/((4*p[i])+r[i]))\n",
    "    \n",
    "    #find optimal threshold\n",
    "    t=thresholds[f2_list.index(max(f2_list))]\n",
    "    \n",
    "    #calculate new labels\n",
    "    new_labels=adjusted_classes(y_probas[:,1],t)\n",
    "    \n",
    "    #calculate f2 score after thresholding\n",
    "    f2_score=fbeta_score(y_test,new_labels,beta=2)\n",
    "    \n",
    "    return f1_score(labels_test,y_pred),f2_score,f2_score_before_thresholding,random_search.best_params_\n",
    "\n",
    "\n",
    "# init variable values and create parameter grid for estimators\n",
    "class_weight='balanced'\n",
    "ratio_w=1\n",
    "\n",
    "svm_grid = {'C': [.01,.1,.4,.6,1,1.5],\n",
    "              'kernel': [ 'rbf', 'poly','linear'],\n",
    "              'gamma':  [.01, .1, 1],\n",
    "              'degree': [1, 2, 3, 4, 5],\n",
    "              'probability': [True],\n",
    "              'class_weight':[class_weight]\n",
    "             }\n",
    "\n",
    "rf_grid={'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False],\n",
    "                'class_weight':['balanced']}\n",
    "\n",
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]\n",
    "\n",
    "\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
