{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, roc_auc_score,matthews_corrcoef, precision_recall_curve\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.2'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scikit-learn.__version__\n",
    "import xgboost as x\n",
    "x.__version__\n",
    "\n",
    "\n",
    "#pandas oudere versie\n",
    "#sklearn iets oudere versie\n",
    "#imbalanced-learn oudere versie\n",
    "#xgboost verouderde versie\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\tsmit\\\\Downloads\\\\KPMG thesis\\\\Data\")\n",
    "\n",
    "dfenron=pd.read_pickle(\"ENRON set.zip\")\n",
    "dfenron['CONTENT+SUBJECT']=dfenron['CONTENT']+dfenron['SUBJECT']\n",
    "dfenron=dfenron[dfenron['CONTENT']!=' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"Z:\\\\Scriptie TSmit\\\\pickles\")\n",
    "os.getcwd()\n",
    "\n",
    "dfwhiskey=pd.read_pickle('Whiskey.zip')\n",
    "dfwhiskey['CONTENT+SUBJECT']=[' '.join(dfwhiskey.loc[index]['TOKENS_BODY_SUBJ']) for index in dfwhiskey.index]\n",
    "dfwhiskey=dfwhiskey[dfwhiskey['CONTENT']!=' ']\n",
    "dfwhiskey=dfwhiskey[dfwhiskey['CONTENT']!='']\n",
    "\n",
    "df2020=pd.read_pickle('2020.zip')\n",
    "df2020['CONTENT+SUBJECT']=[' '.join(df2020.loc[index]['TOKENS_BODY_SUBJ']) for index in df2020.index]\n",
    "df2020=df2020[df2020['CONTENT']!=' ']\n",
    "df2020=df2020[df2020['CONTENT']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop with settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_preprocess_grid(dataset_name):\n",
    "    if dataset_name == 'enron':\n",
    "        smoteenn_grid=[False]\n",
    "        undersampling_grid=[True]\n",
    "    else:\n",
    "        smoteenn_grid=[True]\n",
    "        undersampling_grid=[True,False]\n",
    "    return smoteenn_grid, undersampling_grid\n",
    "\n",
    "\n",
    "#true smoteenn true\n",
    "# undersampling beide\n",
    "# class weigts beide\n",
    "#Sampleweights true\n",
    "\n",
    "def create_score_table(dataset_name):\n",
    "    enron_scores=pd.DataFrame(columns=['Text representation','Word/char','Weighting','Algorithm','Smoteenn','Undersampling','Classweights','Sampleweights'])\n",
    "    a=0\n",
    "    preprocess_grid_smoteenn,preprocess_grid_undersampling=check_preprocess_grid(dataset_name) \n",
    "    for representation in ['UNIGRAM','BIGRAM','TRIGRAM']:\n",
    "        for wordchar in ['WORD','CHAR']:\n",
    "            for weighting in ['INFO-GAIN']:\n",
    "                for model in ['svm','rf','xgb','cnb']:\n",
    "                    for smoteennyes in preprocess_grid_smoteenn:\n",
    "                        for underyes in preprocess_grid_undersampling:  # always !! undersampling for enron \n",
    "                            for classweightyes in [True,False]:\n",
    "                                for sampleweightyes in [True]:\n",
    "                                    enron_scores.loc[a]=[representation,wordchar,weighting,model,smoteennyes,underyes,classweightyes,sampleweightyes]\n",
    "                                    a+=1\n",
    "    return enron_scores\n",
    "\n",
    "\n",
    "#in case of enron training #fill in right datanames en sets!\n",
    "\n",
    "#df2020_training_results=do_training('2020','tokens')\n",
    "\n",
    "trainingwhiskey=create_score_table('2020')\n",
    "#kijken hoe training is op GEEN lege emails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngram(text_representation):\n",
    "    if text_representation=='UNIGRAM':\n",
    "        ngram_range=(1,1)\n",
    "    elif text_representation=='BIGRAM':\n",
    "        ngram_range=(1,2)\n",
    "    elif text_representation=='TRIGRAM':\n",
    "        ngram_range=(1,3)\n",
    "    return ngram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngram(text_representation):\n",
    "    if text_representation=='UNIGRAM':\n",
    "        ngram_range=(1,1)\n",
    "    elif text_representation=='BIGRAM':\n",
    "        ngram_range=(1,2)\n",
    "    elif text_representation=='TRIGRAM':\n",
    "        ngram_range=(1,3)\n",
    "    return ngram_range\n",
    "\n",
    "def perform_tf_idf(X_train,X_test,ngram_range,analyzer,use_idf):\n",
    "    min_df = 10\n",
    "    max_df = 1.\n",
    "    max_features = 300\n",
    "    tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                            analyzer=analyzer,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True,\n",
    "                           use_idf=use_idf)\n",
    "    features_train = tfidf.fit_transform(X_train).toarray()\n",
    "    features_test = tfidf.transform(X_test).toarray()\n",
    "    return features_train,features_test\n",
    "\n",
    "def perform_countvectorizer(X_train,X_test,ngram_range,analyzer):\n",
    "    min_df = 10\n",
    "    max_df = 1.\n",
    "    max_features = 300\n",
    "    countvect = CountVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,analyzer=analyzer,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features)\n",
    "    features_train = countvect.fit_transform(X_train).toarray()\n",
    "    features_test = countvect.transform(X_test).toarray()\n",
    "    return features_train,features_test    \n",
    "\n",
    "def perform_info_gain_df(X_train,X_test,labels,ngram_range,analyzer):\n",
    "    from info_gain import info_gain\n",
    "    X_train=X_train.astype(str)\n",
    "    X_test=X_test.astype(str)\n",
    "    min_df = 10\n",
    "    max_df = 1.\n",
    "    max_features = 300\n",
    "    countvect = CountVectorizer(encoding='utf-8',ngram_range=ngram_range,analyzer=analyzer,stop_words=None,lowercase=False,max_df=max_df,min_df=min_df,max_features=max_features,binary=True)\n",
    "    features_train = countvect.fit_transform(X_train).toarray()\n",
    "    features_test = countvect.transform(X_test).toarray()\n",
    "    \n",
    "    dftrain = pd.DataFrame(features_train, columns=countvect.get_feature_names())\n",
    "    dftest = pd.DataFrame(features_test, columns=countvect.get_feature_names())\n",
    "    \n",
    "    dftrain['RESPONSIVE']=labels\n",
    "    \n",
    "    \n",
    "    ig_dict={}\n",
    "    for column in dftrain.columns:\n",
    "        ig  = info_gain.info_gain(dftrain['RESPONSIVE'], dftrain[column])\n",
    "        ig_dict[column]=ig\n",
    "    \n",
    "    for column in dftrain.columns:\n",
    "        info_gain=ig_dict[column]\n",
    "        dftrain[column]=np.where(dftrain[column] == 0, 0, info_gain)\n",
    "    for column in dftest.columns:\n",
    "        info_gain=ig_dict[column]\n",
    "        dftest[column]=np.where(dftest[column] == 0, 0, info_gain)\n",
    "    return dftrain.loc[:, dftrain.columns != 'RESPONSIVE'],dftest\n",
    "\n",
    "    \n",
    "def train_classifier(data,model,iterations,cv,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['TOKENS'],   #ALLEEN VOOR ENRON\n",
    "                                                    data['RESPONSIVE'], \n",
    "                                                    test_size=0.15)\n",
    "    X_train=X_train.astype(str)\n",
    "    X_test=X_test.astype(str)    #weet niet zeker of dit moet\n",
    "    \n",
    "    ngram_range = extract_ngram(text_representation)\n",
    "    \n",
    "    print(feature_weighting)\n",
    "    if feature_weighting =='TF-IDF':\n",
    "        features_train,features_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),True)\n",
    "    elif feature_weighting =='TF':\n",
    "        features_train,features_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),False)\n",
    "    elif feature_weighting=='BINARY':\n",
    "        features_train,features_test=perform_countvectorizer(X_train,X_test,ngram_range,wordchar.lower())\n",
    "    elif feature_weighting=='INFO-GAIN':\n",
    "        features_train,features_test=perform_info_gain_df(X_train,X_test,y_train,ngram_range,wordchar.lower())\n",
    "    \n",
    "    labels_train = y_train\n",
    "    labels_test = y_test\n",
    "    \n",
    "    print('na vectorizer')\n",
    "    if smoteennyes:\n",
    "        print('start smoteenn')\n",
    "        sme=SMOTEENN()\n",
    "        features_train, labels_train = sme.fit_resample(features_train, labels_train)\n",
    "        print('SMOTEENN: True')\n",
    "        \n",
    "    if underyes:\n",
    "        under=RandomUnderSampler(sampling_strategy=1)\n",
    "        features_train, labels_train =under.fit_resample(features_train,labels_train)\n",
    "        print('Undersamping: True')\n",
    "        \n",
    "    if classweightyes:\n",
    "        class_weight='balanced'\n",
    "    else:\n",
    "        class_weight=None\n",
    "    print('class_weight: ', class_weight)\n",
    "    \n",
    "    if model=='svm':\n",
    "        print('Support vector machines')\n",
    "        estimator= SVC(random_state=8)\n",
    "        random_grid=svm_grid\n",
    "    elif model=='rf':\n",
    "        print('Random forest')\n",
    "        estimator= RandomForestClassifier(random_state=8)\n",
    "        random_grid=rf_grid\n",
    "    elif model=='cnb':\n",
    "        print('Complement Naive Bayes')\n",
    "        estimator= ComplementNB()\n",
    "        random_grid=cnb_grid\n",
    "    elif model=='xgb':\n",
    "        print('XGBoost')\n",
    "        estimator=XGBClassifier(random_state=8)\n",
    "        ratio_w=len(data[data['RESPONSIVE']==0])/len(data[data['RESPONSIVE']==1])\n",
    "        random_grid=xgb_grid\n",
    "    \n",
    "   \n",
    "    random_search = RandomizedSearchCV(estimator=estimator,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=iterations,\n",
    "                                   scoring=ftwo_scorer,\n",
    "                                   cv=cv, \n",
    "                                   verbose=1, \n",
    "                                   refit=True)\n",
    "    if sampleweightyes:\n",
    "        print('Sample weight: True')\n",
    "        sample_weight = compute_sample_weight(class_weight='balanced', y=labels_train)\n",
    "        print('start random search')\n",
    "        random_search.fit(features_train, labels_train,sample_weight=sample_weight)\n",
    "        print('end random search')\n",
    "    else:\n",
    "        print('start random search')\n",
    "        random_search.fit(features_train, labels_train)\n",
    "        print('end random search')\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('Training score:')\n",
    "    print(\"The best hyperparameters from Random Search are:\")\n",
    "    print(random_search.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"The mean f2 of a model with these hyperparameters is:\")\n",
    "    print(random_search.best_score_)\n",
    "    print(\"\")\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('Test score:')\n",
    "    y_pred=random_search.predict(features_test)\n",
    "    y_probas=random_search.predict_proba(features_test)\n",
    "    print(\"\")\n",
    "    print(\"Confusion matrix\")\n",
    "    print(confusion_matrix(labels_test,y_pred))\n",
    "    print(classification_report(labels_test, y_pred))\n",
    "    print('')\n",
    "    print(\"F1: \",f1_score(labels_test,y_pred))\n",
    "    print(\"ROC-AUC: \",roc_auc_score(labels_test,y_pred))\n",
    "    print('Matthews corr coef: ',matthews_corrcoef(labels_test,y_pred))\n",
    "    f2_score_before_thresholding=fbeta_score(labels_test,y_pred,beta=2)\n",
    "    p, r, thresholds = precision_recall_curve(y_test, y_probas[:,1])\n",
    "\n",
    "    f2_list=[]\n",
    "    for i in range(0,len(p)):\n",
    "        f2_list.append((5*p[i]*r[i])/((4*p[i])+r[i]))\n",
    "\n",
    "    t=thresholds[f2_list.index(max(f2_list))]\n",
    "    new_labels=adjusted_classes(y_probas[:,1],t)\n",
    "    f2_score=fbeta_score(y_test,new_labels,beta=2)\n",
    "    \n",
    "    return f1_score(labels_test,y_pred),f2_score,f2_score_before_thresholding,random_search.best_params_\n",
    "\n",
    "class_weight='balanced'\n",
    "ratio_w=1\n",
    "\n",
    "svm_grid = {'C': [.01,.1,.4,.6,1,1.5],\n",
    "              'kernel': [ 'rbf', 'poly','linear'],\n",
    "              'gamma':  [.01, .1, 1],\n",
    "              'degree': [1, 2, 3, 4, 5],\n",
    "              'probability': [True],\n",
    "              'class_weight':[class_weight]\n",
    "             }\n",
    "\n",
    "xgb_grid = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'scale_pos_weight':[ratio_w] \n",
    "}\n",
    "\n",
    "rf_grid={'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False],\n",
    "                'class_weight':['balanced']}\n",
    "\n",
    "cnb_grid={'alpha':[0,0.3,0.6,0.8,1], \n",
    "          'fit_prior':[False,True],\n",
    "          'norm':[False,True]}\n",
    "\n",
    "\n",
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]\n",
    "\n",
    "\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter parameter combination in de estimator!\n",
    "\n",
    "\n",
    "def do_testing(data,model,best_params,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['TOKENS'], \n",
    "                                                    data['RESPONSIVE'], \n",
    "                                                    test_size=0.15,stratify=data['RESPONSIVE'])\n",
    "                                                    \n",
    "    X_train_1, X_val, y_train_1, y_val=train_test_split(X_train,  \n",
    "                                                    y_train, \n",
    "                                                    test_size=0.15,stratify=y_train)               \n",
    "    print('verhouding in train qua labels: ', y_train.value_counts()[0]/y_train.value_counts()[1])\n",
    "    print('verhouding in test qua labels: ', y_test.value_counts()[0]/y_test.value_counts()[1])\n",
    "    \n",
    "    def fit_and_val_model(model,best_params,X_train_1,y_train_1,X_val,y_val,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "        X_train_1=X_train_1.astype(str)\n",
    "        X_val=X_val.astype(str)    #weet niet zeker of dit moet\n",
    "    \n",
    "        ngram_range = extract_ngram(text_representation)\n",
    "        if feature_weighting =='TF-IDF':\n",
    "            X_train_1,X_val=perform_tf_idf(X_train_1,X_val,ngram_range,wordchar.lower(),True)\n",
    "        elif feature_weighting =='TF':\n",
    "            X_train_1,X_val=perform_tf_idf(X_train_1,X_val,ngram_range,wordchar.lower(),False)\n",
    "        elif feature_weighting=='BINARY':\n",
    "            X_train_1,X_val=perform_countvectorizer(X_train_1,X_val,ngram_range,wordchar.lower())\n",
    "        elif feature_weighting=='INFO-GAIN':\n",
    "            X_train_1,X_val=perform_info_gain_df(X_train_1,X_val,y_train_1,ngram_range,wordchar.lower())\n",
    "            \n",
    "        y_train_1 = y_train_1\n",
    "        y_val = y_val\n",
    "        \n",
    "        if smoteennyes:\n",
    "            print('start smoteenn')\n",
    "            sme=SMOTEENN()\n",
    "            X_train_1, y_train_1 = sme.fit_resample(X_train_1, y_train_1)\n",
    "            print('SMOTEENN: True')\n",
    "\n",
    "        if underyes:\n",
    "            under=RandomUnderSampler(sampling_strategy=1)\n",
    "            X_train_1,y_train_1 =under.fit_resample(X_train_1,y_train_1)\n",
    "            print('Undersamping: True')\n",
    "\n",
    "        if classweightyes:\n",
    "            class_weight='balanced'\n",
    "        else:\n",
    "            class_weight=None\n",
    "        print('class_weight: ', class_weight)\n",
    "        \n",
    "        \n",
    "        if model=='svm':\n",
    "            estimator= SVC(C=best_params['C'],kernel=best_params['kernel'],gamma=best_params['gamma'],degree=best_params['degree'],probability=best_params['probability'],class_weight=class_weight)\n",
    "        elif model=='rf':\n",
    "            estimator= RandomForestClassifier(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth'],min_samples_split=best_params['min_samples_split'],min_samples_leaf=best_params['min_samples_leaf'],bootstrap=best_params['bootstrap'],class_weight='balanced')\n",
    "        elif model=='cnb':\n",
    "            estimator= ComplementNB(alpha=best_params['alpha'],fit_prior=best_params['fit_prior'],norm=best_params['norm'])\n",
    "        elif model=='xgb':\n",
    "            ratio_w=y_train_1.value_counts()[0]/y_train_1.value_counts()[1]\n",
    "            estimator=XGBClassifier(min_child_weight=best_params['min_child_weight'],gamma=best_params['gamma'],subsample=best_params['subsample'],colsample_bytree=best_params['colsample_bytree'],max_depth=best_params['max_depth'], scale_pos_weight=ratio_w)\n",
    "         \n",
    "    \n",
    "        if sampleweightyes:\n",
    "            sample_weight = compute_sample_weight(class_weight='balanced', y=y_train_1)\n",
    "            print('start random search')\n",
    "            estimator.fit(X_train_1, y_train_1,sample_weight=sample_weight)\n",
    "            print('end random search')\n",
    "        else:\n",
    "            estimator.fit(X_train_1,y_train_1)\n",
    "\n",
    "        y_pred=estimator.predict(X_val)\n",
    "        y_probas=estimator.predict_proba(X_val)\n",
    "        f2_score_before_thresholding=fbeta_score(y_val,y_pred,beta=2)\n",
    "        p, r, thresholds = precision_recall_curve(y_val, y_probas[:,1])\n",
    "        f2_list=[]\n",
    "        for i in range(0,len(p)):\n",
    "            f2_list.append((5*p[i]*r[i])/((4*p[i])+r[i]))\n",
    "        t=thresholds[f2_list.index(max(f2_list))]\n",
    "        new_labels=adjusted_classes(y_probas[:,1],t)\n",
    "        f2_score=fbeta_score(y_val,new_labels,beta=2)\n",
    "\n",
    "        print('F2 before thresholding: ',f2_score_before_thresholding)\n",
    "        print('F2 after thresholding: ', f2_score)\n",
    "        print(t)\n",
    "        print('----------------------------------')\n",
    "        return t\n",
    "\n",
    "    def test_model(model,best_params,X_train,y_train,X_test,y_test,t,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "        X_train=X_train.astype(str)\n",
    "        X_test=X_test.astype(str)    #weet niet zeker of dit moet\n",
    "\n",
    "        ngram_range = extract_ngram(text_representation)\n",
    "        \n",
    "        if feature_weighting =='TF-IDF':\n",
    "            X_train,X_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),True)\n",
    "        elif feature_weighting =='TF':\n",
    "            X_train,X_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),False)\n",
    "        elif feature_weighting=='BINARY':\n",
    "            X_train,X_test=perform_countvectorizer(X_train,X_test,ngram_range,wordchar.lower())\n",
    "        elif feature_weighting=='INFO-GAIN':\n",
    "            X_train,X_test=perform_info_gain_df(X_train,X_test,y_train,ngram_range,wordchar.lower())   \n",
    "        \n",
    "        y_train = y_train\n",
    "        y_test = y_test\n",
    "        \n",
    "        if smoteennyes:\n",
    "            print('start smoteenn')\n",
    "            sme=SMOTEENN()\n",
    "            X_train, y_train = sme.fit_resample(X_train, y_train)\n",
    "            print('SMOTEENN: True')\n",
    "\n",
    "        if underyes:\n",
    "            under=RandomUnderSampler(sampling_strategy=1)\n",
    "            X_train,y_train =under.fit_resample(X_train,y_train)\n",
    "            print('Undersamping: True')\n",
    "\n",
    "        if classweightyes:\n",
    "            class_weight='balanced'\n",
    "        else:\n",
    "            class_weight=None\n",
    "        print('class_weight: ', class_weight)\n",
    "        \n",
    "        if model=='svm':\n",
    "            estimator= SVC(C=best_params['C'],kernel=best_params['kernel'],gamma=best_params['gamma'],degree=best_params['degree'],probability=best_params['probability'],class_weight=class_weight)\n",
    "        elif model=='rf':\n",
    "            estimator= RandomForestClassifier(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth'],min_samples_split=best_params['min_samples_split'],min_samples_leaf=best_params['min_samples_leaf'],bootstrap=best_params['bootstrap'],class_weight='balanced')\n",
    "        elif model=='cnb':\n",
    "            estimator= ComplementNB(alpha=best_params['alpha'],fit_prior=best_params['fit_prior'],norm=best_params['norm'])\n",
    "        elif model=='xgb':\n",
    "            ratio_w=y_train.value_counts()[0]/y_train.value_counts()[1]\n",
    "            estimator=XGBClassifier(min_child_weight=best_params['min_child_weight'],gamma=best_params['gamma'],subsample=best_params['subsample'],colsample_bytree=best_params['colsample_bytree'],max_depth=best_params['max_depth'], scale_pos_weight=ratio_w)\n",
    "         \n",
    "         \n",
    "        if sampleweightyes:\n",
    "            sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "            estimator.fit(X_train, y_train,sample_weight=sample_weight)\n",
    "        else:\n",
    "            estimator.fit(X_train,y_train)\n",
    "            \n",
    "        y_pred=estimator.predict(X_test)\n",
    "        y_probas=estimator.predict_proba(X_test)\n",
    "        f2_score_without_thresholding=fbeta_score(y_test,y_pred,beta=2)\n",
    "\n",
    "        new_labels=adjusted_classes(y_probas[:,1],t)\n",
    "        f2_score=fbeta_score(y_test,new_labels,beta=2)\n",
    "\n",
    "        print('test f2 without thresholding: ',f2_score_without_thresholding)\n",
    "        print('test f2 after thresholding: ',f2_score)\n",
    "        return f2_score, f2_score_without_thresholding\n",
    "\n",
    "    t=fit_and_val_model(model,best_params,X_train_1,y_train_1,X_val,y_val,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting)\n",
    "    return test_model(model,best_params,X_train,y_train,X_test,y_test,t,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting)\n",
    "\n",
    "\n",
    "#test configuration on multiple test sets to check reliability\n",
    "\n",
    "def testing_statistics(data,model,best_params,iterations,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):  #oneven aantal!!!\n",
    "    f2_thresh_scores=[]\n",
    "    f2_no_thresh_scores=[]\n",
    "    \n",
    "    for iter in range(0,iterations):\n",
    "        f2_thresh,f2_no_thresh=do_testing(data,model,best_params,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting)\n",
    "        f2_thresh_scores.append(f2_thresh)\n",
    "        f2_no_thresh_scores.append(f2_no_thresh)\n",
    "    return f2_thresh_scores, f2_no_thresh_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na train_classifier ook nog alles aanpassen van testing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext representation\tWord/char\tWeighting\tSettings\tSmoteenn\tUndersampling\tClassweights\tSampleweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_results(training_results,data,dataset_name,tijdelijke_index):\n",
    "    #training_results['F2 test with thresholding (mean)']=0\n",
    "    #training_results['F2 test with thresholding (median)']=0\n",
    "    #training_results['F2 test without thresholding (mean)']=0\n",
    "    #training_results['F2 test without thresholding (median)']=0\n",
    "    \n",
    "    training_results['best params']=0\n",
    "    print(len(training_results))\n",
    "    for index in tijdelijke_index[45:50]:\n",
    "\n",
    "        feature_weighting=training_results.loc[index]['Weighting']\n",
    "        word_char=training_results.loc[index]['Word/char']\n",
    "        text_representation=training_results.loc[index]['Text representation']\n",
    "        smoteennyes=training_results.loc[index]['Smoteenn']\n",
    "        underyes=training_results.loc[index]['Undersampling']#==' True ')#change tooo bolean\n",
    "        classweightyes=training_results.loc[index]['Classweights']#==' True ')\n",
    "        sampleweightyes=training_results.loc[index]['Sampleweights']#==' True ')\n",
    "        algorithm=training_results.loc[index]['Algorithm']\n",
    "        from sklearn.metrics import confusion_matrix, f1_score, classification_report, roc_auc_score,matthews_corrcoef, precision_recall_curve\n",
    "        f1_score,f2_score,f2_before,best_params=train_classifier(data,algorithm,30,4,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,word_char,feature_weighting)\n",
    "        f2_thresh_scores,f2_no_thresh_scores=testing_statistics(data,algorithm, best_params,11,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,word_char,feature_weighting)\n",
    "        training_results.loc[index,'F2 test with thresholding (mean)']=np.mean(f2_thresh_scores)\n",
    "        training_results.loc[index,'F2 test with thresholding (median)']=np.median(f2_thresh_scores)\n",
    "        training_results.loc[index,'F2 test without thresholding (mean)']=np.mean(f2_no_thresh_scores)\n",
    "        training_results.loc[index,'F2 test without thresholding (median)']=np.median(f2_no_thresh_scores)\n",
    "        training_results.loc[index,'best params']=[best_params]\n",
    "        os.chdir(\"C:\\\\Users\\\\tsmit\\\\Downloads\\\\KPMG thesis\\\\Results\\\\Results per feature weighting\")\n",
    "        training_results.to_excel('ALMIGHTY 288 laatste 4 resterende rows INFO-GAIN - test results {}.xlsx'.format(dataset_name))\n",
    "        print('--------------------------------------------------------------- {} DONE'.format(index))\n",
    "        #except:\n",
    "            #print('{} ging niet goed-------------------------------------------------'.format(index))\n",
    "    os.chdir(\"C:\\\\Users\\\\tsmit\\\\Downloads\\\\KPMG thesis\\\\Results\\\\Results per feature weighting\")\n",
    "    training_results.to_excel('ALMIGHTY 288 laatste 4 resterende rows INFO-GAIN - test results {}.xlsx'.format(dataset_name)) \n",
    "    return training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## call testing function for testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "INFO-GAIN\n",
      "na vectorizer\n",
      "start smoteenn\n",
      "SMOTEENN: True\n",
      "Undersamping: True\n",
      "class_weight:  None\n",
      "XGBoost\n",
      "Sample weight: True\n",
      "start random search\n",
      "Fitting 4 folds for each of 30 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\tsmit\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: feature_names must be string, and may not contain [, ] or <\n",
      "\n",
      "  FitFailedWarning)\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:    4.4s finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names must be string, and may not contain [, ] or <",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-209-48198e6c232a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfwhiskey_testing_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtesting_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingwhiskey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdfwhiskey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'whiskey'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtijdelijke_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-208-9fdcebb2f66c>\u001b[0m in \u001b[0;36mtesting_results\u001b[1;34m(training_results, data, dataset_name, tijdelijke_index)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Algorithm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmatthews_corrcoef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf2_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf2_before\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msmoteennyes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0munderyes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclassweightyes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msampleweightyes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext_representation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_char\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_weighting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mf2_thresh_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf2_no_thresh_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtesting_statistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msmoteennyes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0munderyes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclassweightyes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msampleweightyes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext_representation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_char\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_weighting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtraining_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'F2 test with thresholding (mean)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf2_thresh_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-125-af9562d9820b>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[1;34m(data, model, iterations, cv, smoteennyes, underyes, classweightyes, sampleweightyes, text_representation, wordchar, feature_weighting)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'start random search'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mrandom_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'end random search'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    813\u001b[0m         train_dmatrix = DMatrix(X, label=training_labels, weight=sample_weight,\n\u001b[0;32m    814\u001b[0m                                 \u001b[0mbase_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m                                 missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m         self._Booster = train(xgb_options, train_dmatrix,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_base_margin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mfeature_names\u001b[1;34m(self, feature_names)\u001b[0m\n\u001b[0;32m    956\u001b[0m                        \u001b[1;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'['\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m']'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'<'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m                        for f in feature_names):\n\u001b[1;32m--> 958\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'feature_names must be string, and may not contain [, ] or <'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    959\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m             \u001b[1;31m# reset feature_types also\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names must be string, and may not contain [, ] or <"
     ]
    }
   ],
   "source": [
    "dfwhiskey_testing_results=testing_results(trainingwhiskey,dfwhiskey,'whiskey',tijdelijke_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2020_testing_results.groupby(['Text representation','Word/char']).max()\n",
    "\n",
    "#a=df2020_testing_results.loc[df2020_testing_results.groupby(['Text representation','Word/char'])[\"F2 test with thresholding (mean)\"].idxmax()]\n",
    "#a.groupby(['Text representation','Word/char']).max()\n",
    "\n",
    "#os.chdir(\"C:\\\\Users\\\\tsmit\\\\Downloads\\\\KPMG thesis\\\\Results\\\\Results per feature weighting\")\n",
    "#best2020=pd.read_excel('ALMIGHTY 288 rows - test results 2020.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "            34, 35, 36, 37, 38, 39, 56, 57, 58, 59, 88, 89, 90, 91],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tijdelijke_index=dfwhiskey_testing_results[dfwhiskey_testing_results['best params']==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([88, 89, 90, 91], dtype='int64')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tijdelijke_index[44:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text representation</th>\n",
       "      <th>Word/char</th>\n",
       "      <th>Weighting</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Smoteenn</th>\n",
       "      <th>Undersampling</th>\n",
       "      <th>Classweights</th>\n",
       "      <th>Sampleweights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>UNIGRAM</td>\n",
       "      <td>WORD</td>\n",
       "      <td>INFO-GAIN</td>\n",
       "      <td>svm</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>UNIGRAM</td>\n",
       "      <td>WORD</td>\n",
       "      <td>INFO-GAIN</td>\n",
       "      <td>svm</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>UNIGRAM</td>\n",
       "      <td>WORD</td>\n",
       "      <td>INFO-GAIN</td>\n",
       "      <td>svm</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>UNIGRAM</td>\n",
       "      <td>WORD</td>\n",
       "      <td>INFO-GAIN</td>\n",
       "      <td>svm</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>UNIGRAM</td>\n",
       "      <td>WORD</td>\n",
       "      <td>INFO-GAIN</td>\n",
       "      <td>rf</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>TRIGRAM</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>INFO-GAIN</td>\n",
       "      <td>xgb</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>TRIGRAM</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>INFO-GAIN</td>\n",
       "      <td>cnb</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>TRIGRAM</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>INFO-GAIN</td>\n",
       "      <td>cnb</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>TRIGRAM</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>INFO-GAIN</td>\n",
       "      <td>cnb</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>TRIGRAM</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>INFO-GAIN</td>\n",
       "      <td>cnb</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Text representation Word/char  Weighting Algorithm Smoteenn Undersampling  \\\n",
       "0              UNIGRAM      WORD  INFO-GAIN       svm     True          True   \n",
       "1              UNIGRAM      WORD  INFO-GAIN       svm     True          True   \n",
       "2              UNIGRAM      WORD  INFO-GAIN       svm     True         False   \n",
       "3              UNIGRAM      WORD  INFO-GAIN       svm     True         False   \n",
       "4              UNIGRAM      WORD  INFO-GAIN        rf     True          True   \n",
       "..                 ...       ...        ...       ...      ...           ...   \n",
       "91             TRIGRAM      CHAR  INFO-GAIN       xgb     True         False   \n",
       "92             TRIGRAM      CHAR  INFO-GAIN       cnb     True          True   \n",
       "93             TRIGRAM      CHAR  INFO-GAIN       cnb     True          True   \n",
       "94             TRIGRAM      CHAR  INFO-GAIN       cnb     True         False   \n",
       "95             TRIGRAM      CHAR  INFO-GAIN       cnb     True         False   \n",
       "\n",
       "   Classweights Sampleweights  \n",
       "0          True          True  \n",
       "1         False          True  \n",
       "2          True          True  \n",
       "3         False          True  \n",
       "4          True          True  \n",
       "..          ...           ...  \n",
       "91        False          True  \n",
       "92         True          True  \n",
       "93        False          True  \n",
       "94         True          True  \n",
       "95        False          True  \n",
       "\n",
       "[96 rows x 8 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingwhiskey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
