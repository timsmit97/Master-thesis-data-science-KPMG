{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, roc_auc_score,matthews_corrcoef, precision_recall_curve\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: M1+M2+..+T1+T2+..+T300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_2(datasets,datasets_meta,datasets_names,best_text_settings):\n",
    "    #create score dataframe\n",
    "    scores_df=pd.DataFrame(columns=['Dataset','Algorithm','Smoten','Undersampling','Classweights','Sampleweights','F2-score (with thresholding','F2-score (without) thresholding','# times meta-feature in top20','# times meta-feature in top10','highest importance'])\n",
    "    \n",
    "    # find best text representation of current email dataset\n",
    "    best_settings=find_best_settings(best_text_settings)\n",
    "    count=0\n",
    "    \n",
    "    #perform scenario 2 for all 3 dataset (iterate 3 times)\n",
    "    for i in range(0,3):\n",
    "        #return data objects\n",
    "        dataset,metadataset,dataset_name,row=return_parts(datasets,datasets_meta,datasets_names,best_settings,i)\n",
    "        metadataset['TOKENS']=dataset['TOKENS']\n",
    "        \n",
    "        #init score lists\n",
    "        f2_scores=[]\n",
    "        f2_scores_no_thresh=[]\n",
    "        \n",
    "        #linear svm for transparency\n",
    "        for model in ['svm']:\n",
    "            for undersampling in [True,False]:\n",
    "                \n",
    "                #init variables that count how many times metafeatures occur in top20 and top 10features of model\n",
    "                times_meta_features_top20=0\n",
    "                times_meta_features_top10=0\n",
    "                highest_importance=0\n",
    "                \n",
    "                #validate 100 times\n",
    "                for validations in range(0,100):\n",
    "                    \n",
    "                    #split train test\n",
    "                    X_train,X_test,y_train,y_test=train_test_split(metadataset.drop('RESPONSIVE',axis=1),metadataset['RESPONSIVE'], test_size=0.15)\n",
    "                    \n",
    "                    #train classifier\n",
    "                    f2_score_train,f2_no_thresh_train,best_params,random_search=train_classifier(X_train,y_train,model,2,2,True,False,row['Text representation'],row['Word/char'],row['Weighting'])\n",
    "                    \n",
    "                    #test and validate classifier\n",
    "                    f2_score,f2_no_thresh,estimator,columns=test_classifier(model,best_params,X_train,y_train,X_test,y_test,random_search,undersampling,row['Text representation'],row['Word/char'],row['Weighting'])\n",
    "                \n",
    "                    #check for importances in top20 and top10\n",
    "                    times_meta_features_top20+=check_importances(columns,estimator,model,best_params)[0]\n",
    "                    times_meta_features_top10+=check_importances(columns,estimator,model,best_params)[1]\n",
    "                    \n",
    "                    #check if this is the highest importance\n",
    "                    if check_importances(columns,estimator,model,best_params)[2]>highest_importance:\n",
    "                        highest_importance=check_importances(columns,estimator,model,best_params)[2]\n",
    "\n",
    "                    #update score lists\n",
    "                    f2_scores.append(f2_score)\n",
    "                    f2_scores_no_thresh.append(f2_no_thresh)\n",
    "                \n",
    "                #update scores dataframe\n",
    "                scores_df.loc[len(scores_df)]=[dataset_name,model,True,undersampling,True,True,np.mean(f2_scores),np.mean(f2_scores_no_thresh),times_meta_features_top20,times_meta_features_top10,highest_importance]\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model,best_params,X_train,y_train,X_test,y_test,random_search,undersampling,text_representation,wordchar,weighting):\n",
    "    text_train,text_test=obtain_text_representation(X_train['TOKENS'].astype(str),X_test['TOKENS'].astype(str),text_representation,wordchar,weighting)\n",
    "    X_train=pd.concat([X_train.drop('TOKENS',axis=1), text_train], axis = 1)\n",
    "    X_test=pd.concat([X_test.drop('TOKENS',axis=1), text_test], axis = 1)\n",
    "    \n",
    "    sme=SMOTEENN()\n",
    "    X_train, y_train = sme.fit_resample(X_train,y_train)\n",
    "    if undersampling:\n",
    "        under=RandomUnderSampler(sampling_strategy=1)\n",
    "        X_train, y_train =under.fit_resample(X_train,y_train)\n",
    "    class_weight='balanced'\n",
    "    \n",
    "    model=fill_in_algo_params(model,best_params,ratio_w)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    y_probas=model.predict_proba(X_test)\n",
    "    \n",
    "    f2_score_before_thresholding=fbeta_score(y_test,y_pred,beta=2)\n",
    "    print('f2 score before thresholding {}'.format(f2_score_before_thresholding))\n",
    "    p, r, thresholds = precision_recall_curve(y_test, y_probas[:,1])\n",
    "\n",
    "    f2_list=[]\n",
    "    for i in range(0,len(p)):\n",
    "        f2_list.append((5*p[i]*r[i])/((4*p[i])+r[i]))\n",
    "\n",
    "    t=thresholds[f2_list.index(max(f2_list))]\n",
    "    new_labels=adjusted_classes(y_probas[:,1],t)\n",
    "    f2_score=fbeta_score(y_test,new_labels,beta=2)\n",
    "    \n",
    "    return f2_score,f2_score_before_thresholding,model,X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(data,model,iterations,cv,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "    #this function does al preprocessing steps and trains a given classifier in a standardized way\n",
    "    \n",
    "    #split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['TOKENS'],   #ALLEEN VOOR ENRON\n",
    "                                                    data['RESPONSIVE'], \n",
    "                                                    test_size=0.15)\n",
    "    X_train=X_train.astype(str)\n",
    "    X_test=X_test.astype(str)    \n",
    "    \n",
    "    #extract n-grams from the text\n",
    "    ngram_range = extract_ngram(text_representation)\n",
    "    \n",
    "    # perform a text representation method on the data\n",
    "    if feature_weighting =='TF-IDF':\n",
    "        features_train,features_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),True)\n",
    "    elif feature_weighting =='TF':\n",
    "        features_train,features_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),False)\n",
    "    elif feature_weighting=='BINARY':\n",
    "        features_train,features_test=perform_countvectorizer(X_train,X_test,ngram_range,wordchar.lower())\n",
    "    elif feature_weighting=='INFO-GAIN':\n",
    "        features_train,features_test=perform_info_gain_df(X_train,X_test,y_train,ngram_range,wordchar.lower())\n",
    "    \n",
    "    labels_train = y_train\n",
    "    labels_test = y_test\n",
    "    \n",
    "    #determine usage of SMOTE\n",
    "    if smoteennyes:\n",
    "        features_train, labels_train = SMOTEENN().fit_resample(features_train, labels_train)\n",
    "        \n",
    "    #determine usage of undersampling    \n",
    "    if underyes:\n",
    "        features_train, labels_train =RandomUnderSampler(sampling_strategy=1).fit_resample(features_train,labels_train)\n",
    "    \n",
    "    #determine usage of class weights\n",
    "    if classweightyes:\n",
    "        class_weight='balanced'\n",
    "    else:\n",
    "        class_weight=None\n",
    "    \n",
    "    #initialize estimator\n",
    "    if model=='svm':\n",
    "        estimator= SVC(random_state=8)\n",
    "        random_grid=svm_grid\n",
    "    elif model=='rf':\n",
    "        estimator= RandomForestClassifier(random_state=8)\n",
    "        random_grid=rf_grid\n",
    "    elif model=='cnb':\n",
    "        estimator= ComplementNB()\n",
    "        random_grid=cnb_grid\n",
    "    elif model=='xgb':\n",
    "        estimator=XGBClassifier(random_state=8)\n",
    "        ratio_w=len(data[data['RESPONSIVE']==0])/len(data[data['RESPONSIVE']==1])\n",
    "        random_grid=xgb_grid\n",
    "    \n",
    "    # init the random search algorithm\n",
    "    random_search = RandomizedSearchCV(estimator=estimator,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=iterations,\n",
    "                                   scoring=ftwo_scorer,\n",
    "                                   cv=cv, \n",
    "                                   verbose=1, \n",
    "                                   refit=True)\n",
    "    \n",
    "    #check for usage of sample weights and fit random search algorithm\n",
    "    if sampleweightyes:\n",
    "        sample_weight = compute_sample_weight(class_weight='balanced', y=labels_train)\n",
    "        random_search.fit(features_train, labels_train,sample_weight=sample_weight)\n",
    "    else:\n",
    "        random_search.fit(features_train, labels_train)\n",
    "   \n",
    "    y_pred=random_search.predict(features_test)\n",
    "    y_probas=random_search.predict_proba(features_test)\n",
    "    \n",
    "    f2_score_before_thresholding=fbeta_score(labels_test,y_pred,beta=2)\n",
    "    p, r, thresholds = precision_recall_curve(y_test, y_probas[:,1])\n",
    "\n",
    "    f2_list=[]\n",
    "    \n",
    "    #calculate f2 scores for each threshold\n",
    "    for i in range(0,len(p)):\n",
    "        f2_list.append((5*p[i]*r[i])/((4*p[i])+r[i]))\n",
    "    \n",
    "    #find optimal threshold\n",
    "    t=thresholds[f2_list.index(max(f2_list))]\n",
    "    \n",
    "    #calculate new labels\n",
    "    new_labels=adjusted_classes(y_probas[:,1],t)\n",
    "    \n",
    "    #calculate f2 score after thresholding\n",
    "    f2_score=fbeta_score(y_test,new_labels,beta=2)\n",
    "    \n",
    "    return f1_score(labels_test,y_pred),f2_score,f2_score_before_thresholding,random_search.best_params_\n",
    "\n",
    "class_weight='balanced'\n",
    "ratio_w=1\n",
    "\n",
    "svm_grid = {'C': [.01,.1,.4,.6,1,1.5],\n",
    "              'kernel': [ 'rbf', 'poly','linear'],\n",
    "              'gamma':  [.01, .1, 1],\n",
    "              'degree': [1, 2, 3, 4, 5],\n",
    "              'probability': [True],\n",
    "              'class_weight':[class_weight]\n",
    "             }\n",
    "\n",
    "xgb_grid = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'scale_pos_weight':[ratio_w] \n",
    "}\n",
    "\n",
    "rf_grid={'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False],\n",
    "                'class_weight':['balanced']}\n",
    "\n",
    "cnb_grid={'alpha':[0,0.3,0.6,0.8,1], \n",
    "          'fit_prior':[False,True],\n",
    "          'norm':[False,True]}\n",
    "\n",
    "\n",
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]\n",
    "\n",
    "\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_importances(columns, estimator,model,best_params):\n",
    "    #check if there are metafeatures in top 10/20 features \n",
    "    \n",
    "    #init variables and dataframe\n",
    "    return_20=0\n",
    "    return_10=0\n",
    "    highest_importance=0\n",
    "    importances=pd.DataFrame()\n",
    "    importances['Feature names']=columns\n",
    "     \n",
    "    if model=='svm':\n",
    "        #obtain importances\n",
    "        importances['Importance']=estimator.feature_importances_\n",
    "        \n",
    "        #check if in top20\n",
    "        if 20-len(set(importances.sort_values('Importance',ascending=False)[0:20]['Feature names']).difference(set(meta_feature_names)))>0:\n",
    "           # print(20-len(set(importances.sort_values('Importance',ascending=False)[0:20]['Feature names']).difference(set(meta_feature_names))))\n",
    "            return_20=1\n",
    "            highest_importance=importances[importances['Feature names'].isin(meta_feature_names)].head(1)['Importance']\n",
    "            \n",
    "        #check if in top10\n",
    "        if 10-len(set(importances.sort_values('Importance',ascending=False)[0:10]['Feature names']).difference(set(meta_feature_names)))>0:\n",
    "            return_10=1\n",
    "            highest_importance=importances[importances['Feature names'].isin(meta_feature_names)].head(1)['Importance']\n",
    "    \n",
    "        #check what the importance is\n",
    "        if not isinstance(highest_importance, int):\n",
    "            return return_10,return_20,highest_importance.loc[0]\n",
    "    return return_10,return_20,highest_importance\n",
    "\n",
    "        \n",
    "def fill_in_algo_params(model,best_params,ratio_w):\n",
    "    #init algorithms and their best parameters according to the text classifiers\n",
    "    if model=='svm':\n",
    "        estimator= SVC(C=best_params['C'],kernel=best_params['kernel'],gamma=best_params['gamma'],degree=best_params['degree'],probability=best_params['probability'],class_weight=class_weight)\n",
    "    elif model=='rf':\n",
    "        estimator= RandomForestClassifier(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth'],min_samples_split=best_params['min_samples_split'],min_samples_leaf=best_params['min_samples_leaf'],bootstrap=best_params['bootstrap'],class_weight='balanced')\n",
    "    elif model=='cnb':\n",
    "        estimator= ComplementNB(alpha=best_params['alpha'],fit_prior=best_params['fit_prior'],norm=best_params['norm'])\n",
    "    elif model=='xgb':\n",
    "        estimator=XGBClassifier(min_child_weight=best_params['min_child_weight'],gamma=best_params['gamma'],subsample=best_params['subsample'],colsample_bytree=best_params['colsample_bytree'],max_depth=best_params['max_depth'], scale_pos_weight=ratio_w)\n",
    "    return estimator\n",
    "\n",
    "def return_parts(datasets,datasets_meta,datasets_names,best_settings,i):\n",
    "    #return correct data objects\n",
    "    dataset=datasets[i]\n",
    "    metadataset=datasets_meta[i]\n",
    "    dataset_name=datasets_names[i]\n",
    "    row=best_settings[i]\n",
    "    return dataset,metadataset,dataset_name,row\n",
    "\n",
    "def find_best_settings(best_settings):\n",
    "    #find best parameter configuration\n",
    "    row_2020=best_text_settings['2020'].loc[best_text_settings['2020']['F2-score'].idxmax()]    \n",
    "    row_whiskey=best_text_settings['whiskey'].loc[best_text_settings['whiskey']['F2-score'].idxmax()]\n",
    "    row_enron=best_text_settings['enron'].loc[best_text_settings['enron']['F2-score'].idxmax()]\n",
    "    return [row_2020,row_whiskey,row_enron]\n",
    "\n",
    "\n",
    "def obtain_text_representation(X_train,X_test,representation,wordchar,weighting):\n",
    "    #transform text data to the correct representation\n",
    "    ngram_range=extract_ngram(representation)\n",
    "    if weighting=='TF':\n",
    "        X_train,X_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),False)\n",
    "    elif weighting=='TF-IDF':\n",
    "        X_train,X_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),True)\n",
    "    return X_train,X_test\n",
    "\n",
    "def extract_ngram(text_representation):\n",
    "    #obtain correct format \n",
    "    \n",
    "    if text_representation=='UNIGRAM':\n",
    "        ngram_range=(1,1)\n",
    "    elif text_representation=='BIGRAM':\n",
    "        ngram_range=(1,2)\n",
    "    elif text_representation=='TRIGRAM':\n",
    "        ngram_range=(1,3)\n",
    "    return ngram_range\n",
    "\n",
    "def perform_tf_idf(X_train,X_test,ngram_range,analyzer,use_idf):\n",
    "    #perform tf-idf vectorization\n",
    "    \n",
    "    min_df = 10\n",
    "    max_df = 1.\n",
    "    max_features = 300\n",
    "    tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                            analyzer=analyzer,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True,\n",
    "                           use_idf=use_idf)\n",
    "    features_train = tfidf.fit_transform(X_train).toarray()\n",
    "    features_test = tfidf.transform(X_test).toarray()\n",
    "    features_train=pd.DataFrame(features_train, columns=tfidf.get_feature_names(),index=X_train.index)\n",
    "    features_test=pd.DataFrame(features_test, columns=tfidf.get_feature_names(),index=X_test.index)\n",
    "    return features_train,features_test\n",
    "\n",
    "def which_algorithm(model,y_train_1):\n",
    "    #init estimator for train_classifier function\n",
    "    \n",
    "    if model=='svm':\n",
    "        estimator= SVC(random_state=8)\n",
    "        random_grid=svm_grid\n",
    "        ratio_w=None\n",
    "    elif model=='rf':\n",
    "        estimator= RandomForestClassifier(random_state=8)\n",
    "        random_grid=rf_grid\n",
    "        ratio_w=None\n",
    "    elif model=='cnb':\n",
    "        estimator= ComplementNB()\n",
    "        ratio_w=None\n",
    "        random_grid=cnb_grid\n",
    "    elif model=='xgb':\n",
    "        estimator=XGBClassifier(random_state=8)\n",
    "        ratio_w=y_train_1.value_counts()[0]/y_train_1.value_counts()[1]\n",
    "        random_grid=xgb_grid\n",
    "    return estimator,random_grid, ratio_w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
