{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, roc_auc_score,matthews_corrcoef, precision_recall_curve\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Combine optimal classifier with meta-features (stacking) Scenario 1 : M1+M2+..+T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create integrated metafeatures+text prediction dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions from text classifier per row and save it as feature\n",
    "def stack_and_validate(datasets,datasets_meta, datasets_names):\n",
    "    \n",
    "    score_df=pd.DataFrame(columns=['Name metadataset','SMOTEENN','Undersampling','Classweights','Sampleweights','F2-score','roc_auc score','precision','fpr','fnr'])\n",
    "    count_dataset_name=0\n",
    "    count=0\n",
    "    for dataset_meta in datasets_meta:\n",
    "        \n",
    "        for undersampling in [True]:\n",
    "            \n",
    "            #initialize score lists\n",
    "            f2_scores=[]\n",
    "            roc_auc_scores=[]\n",
    "            precision_scores=[]\n",
    "            fpr_scores=[]\n",
    "            fnr_scores=[]\n",
    "            \n",
    "            #test iterations\n",
    "            for i in range(0,20):\n",
    "                \n",
    "                #obtain stacked dataset\n",
    "                stacked=stack_and_obtain(datasets[count_dataset_name],datasets_names[count_dataset_name])\n",
    "                \n",
    "                #integrate with metadata\n",
    "                X_train_meta,y_train_meta,X_test_meta,y_test_meta=create_integrated_dataframe(datasets[count_dataset_name],datasets_names[count_dataset_name],dataset_meta,stacked)\n",
    "    \n",
    "                #prepare integrated dataset for training\n",
    "                X_train_meta,y_train_meta,class_weight=do_prep_steps(X_train_meta,y_train_meta,True,undersampling,True)\n",
    "            \n",
    "                #init combiner function (logistic regression)\n",
    "                estimator= LogisticRegression(random_state=0)\n",
    "                \n",
    "                #init random search algorithm and fit on data\n",
    "                random_search = RandomizedSearchCV(estimator=estimator, param_distributions=logreg_grid,n_iter=10,scoring=ftwo_scorer, cv=4, verbose=1, refit=True)\n",
    "                random_search.fit(X_train_meta, y_train_meta)\n",
    "                \n",
    "                #predict labels and probabilities\n",
    "                y_pred=random_search.predict(X_test_meta)\n",
    "                y_probas=random_search.predict_proba(X_test_meta)\n",
    "                \n",
    "                #calculate f2 score before thresholding and save in score list\n",
    "                f2_score=fbeta_score(y_test_meta,y_pred,beta=2)\n",
    "                f2_scores.append(f2_score)\n",
    "                \n",
    "                #save roc-auc score, precision, fpr and fnr\n",
    "                roc_auc_scores.append(roc_auc_score(y_test_meta,y_probas[:,1]))\n",
    "                tn, fp, fn, tp = confusion_matrix(y_test_meta,y_pred).ravel()\n",
    "                precision=tp/(tp+fp)\n",
    "                fpr = fp/(fp+tn)\n",
    "                fnr=fn/(fn+tp)\n",
    "                \n",
    "                #save coefficients of features in combiner function for explanation\n",
    "                indexje=np.argmax(random_search.best_estimator_.coef_)\n",
    "                coefficients = pd.concat([pd.DataFrame(X_train_meta.columns),pd.DataFrame(np.transpose(random_search.best_estimator_.coef_))], axis = 1)\n",
    "                \n",
    "                #save scores in score lists\n",
    "                precision_scores.append(precision)\n",
    "                fpr_scores.append(fpr)\n",
    "                fnr_scores.append(fnr)\n",
    "           \n",
    "                score=np.max(f2_scores)\n",
    "            \n",
    "            score_df.loc[count]=[datasets_names[count_dataset_name],True,undersampling,True,False,np.mean(f2_scores),np.mean(roc_auc_scores),np.mean(precision_scores),np.mean(fpr_scores),np.mean(fnr_scores)]\n",
    "            count+=1\n",
    "        count_dataset_name+=1\n",
    "    return score_df\n",
    "\n",
    "# parameter grid for logistic regression combiner function\n",
    "logreg_grid={'penalty' : ['l1', 'l2'],\n",
    "    'C' : np.logspace(-4, 4, 20),\n",
    "    'solver' : ['liblinear']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_integrated_dataframe(dataset,dataset_name,dataset_meta,stacked):\n",
    "    #integrate metadata and text predictions\n",
    "    \n",
    "    #call stacked dataframe\n",
    "    stacked=stack_and_obtain(dataset,dataset_name)\n",
    "    \n",
    "    optimal_index=pd.to_numeric(stacked['F2-score']).idxmax()\n",
    "    X_train_meta=dataset_meta.loc[stacked.loc[optimal_index]['train_ids'],dataset_meta.columns != 'RESPONSIVE'] #hierietsveranderd\n",
    "    y_train_meta=dataset_meta.loc[stacked.loc[optimal_index]['train_ids'],dataset_meta.columns == 'RESPONSIVE']\n",
    "    X_test_meta=dataset_meta.loc[stacked.loc[optimal_index]['test_ids'],dataset_meta.columns != 'RESPONSIVE']\n",
    "    y_test_meta=dataset_meta.loc[stacked.loc[optimal_index]['test_ids'],dataset_meta.columns == 'RESPONSIVE']\n",
    "\n",
    "    X_train_meta['Text predictions']=stacked.loc[optimal_index]['train pred']\n",
    "    X_test_meta['Text predictions']=stacked.loc[optimal_index]['test pred']\n",
    "    \n",
    "    return X_train_meta,y_train_meta,X_test_meta,y_test_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_and_obtain(dataset,dataset_name):\n",
    "    #obtain text classifier predictions and stack on metadata-features\n",
    "    \n",
    "    #initialize columns\n",
    "    optimal_configs=optimal_dict[dataset_name]\n",
    "    optimal_configs['train pred']=''\n",
    "    optimal_configs['test pred']=''\n",
    "    optimal_configs['pogingen']=''\n",
    "    optimal_configs['test_score']=''\n",
    "    optimal_configs['train_ids']=''\n",
    "    optimal_configs['test_ids']=''\n",
    "    optimal_configs=optimal_configs.astype('object')\n",
    "  \n",
    "    optimal_index=pd.to_numeric(optimal_configs['F2-score']).idxmax()\n",
    "    \n",
    "    row=optimal_configs.loc[optimal_index]\n",
    "    index=optimal_index\n",
    "    test_f2=0\n",
    "    pogingen=0\n",
    "    \n",
    "    foutmarge=row['F2-score']-0.05\n",
    "   \n",
    "    while test_f2<row['F2-score']-foutmarge:\n",
    "        pogingen+=1\n",
    "        \n",
    "        #split train/test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset['TOKENS'], dataset['RESPONSIVE'], test_size=0.15,stratify=dataset['RESPONSIVE'])\n",
    "        \n",
    "        #save indices\n",
    "        optimal_configs.loc[index,'train_ids']=X_train.index\n",
    "        optimal_configs.loc[index,'test_ids']=X_test.index\n",
    "        \n",
    "        #extract n-gram representations\n",
    "        ngram_range = extract_ngram(row['Text representation'])\n",
    "        \n",
    "        #apply feature weighting\n",
    "        X_train,X_test=perform_feature_weighting(row['Weighting'],X_train,X_test,ngram_range,row['Word/char'].lower())\n",
    "         \n",
    "        #obtain predictions and stack them on metadata\n",
    "        test_pred,train_pred,test_f2=stacking(X_train,y_train,X_test,y_test,8,row['Sampleweights'],row)\n",
    "        \n",
    "        #save predictions\n",
    "        X_train['Predicted']=train_pred\n",
    "        X_test['Predicted']=test_pred\n",
    "        \n",
    "    #update database\n",
    "    optimal_configs.loc[index,'train pred']=train_pred\n",
    "    optimal_configs.loc[index,'test pred']=test_pred\n",
    "    optimal_configs.loc[index,'pogingen']=str(pogingen)\n",
    "    optimal_configs.loc[index,'test_score']=str(test_f2)\n",
    "\n",
    "    return optimal_configs\n",
    "\n",
    "def stacking(train,y,test,y_test,n_fold,sampleweightyes,row):\n",
    "    folds=StratifiedKFold(n_splits=n_fold,random_state=1)\n",
    "    test_pred=np.empty((test.shape[0],1),float)\n",
    "    train_pred=np.empty((0,1),float)\n",
    "    j=0\n",
    "    for train_indices,val_indices in folds.split(train,y.values):\n",
    "        j+=1\n",
    "        \n",
    "        #slice out indices\n",
    "        X_train,X_val=train.iloc[train_indices],train.iloc[val_indices]\n",
    "        y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]\n",
    "        \n",
    "        #prepare datasets for training\n",
    "        X_train,y_train,class_weight=do_prep_steps(X_train,y_train,row['Smoteenn'],row['Undersampling'],row['Classweights'])\n",
    "        \n",
    "        #select correct model\n",
    "        model=select_model(y_train,row['best params'],class_weight,row['Algorithm'])\n",
    "        \n",
    "        #fit model\n",
    "        model=fit_model(sampleweightyes,model,X_train,y_train)\n",
    "        \n",
    "        #predict labels and probabilities\n",
    "        y_pred=model.predict(X_val)    \n",
    "        y_probas=model.predict_proba(X_val)\n",
    "        \n",
    "        #find opt threshold\n",
    "        threshold=find_optimal_threshold(y_val,y_probas)\n",
    "        \n",
    "        #predict new labels\n",
    "        new_labels=adjusted_classes(y_probas[:,1],threshold)\n",
    "        \n",
    "        #f2 score after thresholding\n",
    "        f2_score=fbeta_score(y_val,new_labels,beta=2)\n",
    "        \n",
    "        #append text predictions to use it as feature later on\n",
    "        train_pred=np.append(train_pred,y_probas[:,1])\n",
    "        \n",
    "        #predict on test\n",
    "        if j==n_fold:\n",
    "            test_pred,test_f2=predict_on_test(model,train,test,y,sampleweightyes,y_test)\n",
    "    return test_pred,train_pred,test_f2\n",
    "\n",
    "\n",
    "def predict_on_test(model,X_train,X_test,y_train,sampleweightyes,y_test):\n",
    "    #train en validate\n",
    "    X_train_1, X_val, y_train_1, y_val=train_test_split(X_train,y_train,test_size=0.15,stratify=y_train)    \n",
    "    model=fit_model(sampleweightyes,model,X_train_1,y_train_1)\n",
    "    y_probas=model.predict_proba(X_val)\n",
    "    t=find_optimal_threshold(y_val,y_probas)\n",
    "    \n",
    "    #found t, now train on train and predict on test using t\n",
    "    model=fit_model(sampleweightyes,model,X_train,y_train)\n",
    "    y_probas=model.predict_proba(X_test)\n",
    "    test_labels=adjusted_classes(y_probas[:,1],t)\n",
    "    test_f2=fbeta_score(y_test,test_labels,beta=2)\n",
    "    return y_probas[:,1],test_f2\n",
    "\n",
    "#initialize f2 scorer function\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "    \n",
    "    \n",
    "def fit_model(sampleweightyes,model, X_train,y_train):\n",
    "    if sampleweightyes:\n",
    "        sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "        model.fit(X_train, y_train,sample_weight=sample_weight)\n",
    "    else:\n",
    "        model.fit(X_train,y_train)\n",
    "    return model\n",
    "    \n",
    "def find_optimal_threshold(y_val,y_probas):\n",
    "    p, r, thresholds = precision_recall_curve(y_val, y_probas[:,1])\n",
    "    f2_list=[]\n",
    "    for i in range(0,len(p)):\n",
    "        f2_list.append((5*p[i]*r[i])/((4*p[i])+r[i]))\n",
    "    t=thresholds[f2_list.index(max(f2_list))]\n",
    "    return t\n",
    "    \n",
    "def perform_feature_weighting(feature_weighting,X_train,X_val,ngram_range,wordchar):\n",
    "    X_train=X_train.astype(str)\n",
    "    X_val=X_val.astype(str)\n",
    "    \n",
    "    # perform a text representation method on the data\n",
    "    if feature_weighting =='TF-IDF':\n",
    "        X_train,X_val=perform_tf_idf(X_train,X_val,ngram_range,wordchar,True)\n",
    "    elif feature_weighting =='TF':\n",
    "        X_train,X_val=perform_tf_idf(X_train,X_val,ngram_range,wordchar,False)\n",
    "    elif feature_weighting=='BINARY':\n",
    "        X_train,X_val=perform_countvectorizer(X_train,X_val,ngram_range,wordchar)\n",
    "    else:\n",
    "        print('Feature weighting niet gelukt')\n",
    "    return X_train,X_val\n",
    "\n",
    "def do_prep_steps(X_train,y_train,smoteennyes,underyes,classweightyes):\n",
    "    #determine usage of SMOTE\n",
    "    if smoteennyes:\n",
    "        features_train, labels_train = SMOTEENN().fit_resample(X_train,y_train)\n",
    "        \n",
    "    #determine usage of undersampling    \n",
    "    if underyes:\n",
    "        features_train, labels_train =RandomUnderSampler(sampling_strategy=1).fit_resample(X_train,y_train)\n",
    "    \n",
    "    #determine usage of class weights\n",
    "    if classweightyes:\n",
    "        class_weight='balanced'\n",
    "    else:\n",
    "        class_weight=None\n",
    "    return X_train,y_train,class_weight\n",
    "\n",
    "def select_model(y_train,best_params,class_weight,algorithm):\n",
    "    #initialize estimator\n",
    "    if algorithm=='svm':\n",
    "        estimator= SVC(C=best_params['C'],kernel=best_params['kernel'],gamma=best_params['gamma'],degree=best_params['degree'],probability=best_params['probability'],class_weight=class_weight)\n",
    "    elif algorithm=='rf':\n",
    "        estimator= RandomForestClassifier(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth'],min_samples_split=best_params['min_samples_split'],min_samples_leaf=best_params['min_samples_leaf'],bootstrap=best_params['bootstrap'],class_weight='balanced')\n",
    "    elif algorithm=='cnb':\n",
    "        estimator= ComplementNB(alpha=best_params['alpha'],fit_prior=best_params['fit_prior'],norm=best_params['norm'])\n",
    "    elif algorithm=='xgb':\n",
    "        ratio_w=y_train.value_counts()[0]/y_train.value_counts()[1]\n",
    "        estimator=XGBClassifier(min_child_weight=best_params['min_child_weight'],gamma=best_params['gamma'],subsample=best_params['subsample'],colsample_bytree=best_params['colsample_bytree'],max_depth=best_params['max_depth'], scale_pos_weight=ratio_w)\n",
    "    return estimator\n",
    "\n",
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngram(text_representation):\n",
    "    #transform string text representation to suitable format for sklearn vectorizers\n",
    "    if text_representation=='UNIGRAM':\n",
    "        ngram_range=(1,1)\n",
    "    elif text_representation=='BIGRAM':\n",
    "        ngram_range=(1,2)\n",
    "    elif text_representation=='TRIGRAM':\n",
    "        ngram_range=(1,3)\n",
    "    return ngram_range\n",
    "\n",
    "def perform_tf_idf(X_train,X_test,ngram_range,analyzer,use_idf):\n",
    "    #perform tfidf vectorization \n",
    "    \n",
    "    #initialize values and tfidf-vectorizer\n",
    "    min_df,max_df,max_features = 10, 1., 300\n",
    "    tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                            analyzer=analyzer,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True,\n",
    "                           use_idf=use_idf)\n",
    "    \n",
    "    #transform text data to tfidf representation\n",
    "    features_train = tfidf.fit_transform(X_train).toarray()\n",
    "    features_test = tfidf.transform(X_test).toarray()\n",
    "    return features_train,features_test\n",
    "\n",
    "def perform_countvectorizer(X_train,X_test,ngram_range,analyzer):\n",
    "    #initialize values for countvectorizer\n",
    "    min_df,max_df,max_features = 10, 1., 300\n",
    "   \n",
    "    countvect = CountVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,analyzer=analyzer,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features)\n",
    "    \n",
    "    #transform text data to binary features\n",
    "    features_train = countvect.fit_transform(X_train).toarray()\n",
    "    features_test = countvect.transform(X_test).toarray()\n",
    "    return features_train,features_test    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_testing(data,model,best_params,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "    # testing function that does unique train/test split and applies already found parameters for validation\n",
    "    \n",
    "    #split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['TOKENS'], \n",
    "                                                    data['RESPONSIVE'], \n",
    "                                                    test_size=0.15,stratify=data['RESPONSIVE'])\n",
    "                                                    \n",
    "    #split train/val\n",
    "    X_train_1, X_val, y_train_1, y_val=train_test_split(X_train,  \n",
    "                                                    y_train, \n",
    "                                                    test_size=0.15,stratify=y_train)               \n",
    "    \n",
    "    \n",
    "    def fit_and_val_model(model,best_params,X_train_1,y_train_1,X_val,y_val,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "        #fit and validation function\n",
    "        \n",
    "        \n",
    "        X_train_1,X_val=X_train_1.astype(str),X_val.astype(str) \n",
    "        \n",
    "        #extract ngram representation\n",
    "        ngram_range = extract_ngram(text_representation)\n",
    "        \n",
    "        #perform vectorization of text data\n",
    "        if feature_weighting =='TF-IDF':\n",
    "            X_train_1,X_val=perform_tf_idf(X_train_1,X_val,ngram_range,wordchar.lower(),True)\n",
    "        elif feature_weighting =='TF':\n",
    "            X_train_1,X_val=perform_tf_idf(X_train_1,X_val,ngram_range,wordchar.lower(),False)\n",
    "        elif feature_weighting=='BINARY':\n",
    "            X_train_1,X_val=perform_countvectorizer(X_train_1,X_val,ngram_range,wordchar.lower())\n",
    "        elif feature_weighting=='INFO-GAIN':\n",
    "            X_train_1,X_val=perform_info_gain_df(X_train_1,X_val,y_train_1,ngram_range,wordchar.lower())\n",
    "            \n",
    "        y_train_1,y_val = y_train_1,y_val\n",
    "        \n",
    "        #check whether SMOTE has to be performed\n",
    "        if smoteennyes:\n",
    "            X_train_1, y_train_1 = SMOTEENN().fit_resample(X_train_1, y_train_1)\n",
    "        \n",
    "        #check whether undersampling has to be performed\n",
    "        if underyes:\n",
    "            X_train_1,y_train_1 =RandomUnderSampler(sampling_strategy=1).fit_resample(X_train_1,y_train_1)\n",
    "        \n",
    "        #check whether classweights have to be applied\n",
    "        if classweightyes:\n",
    "            class_weight='balanced'\n",
    "        else:\n",
    "            class_weight=None\n",
    "        \n",
    "        #check which estimator will be used and use optimal parameters\n",
    "        if model=='svm':\n",
    "            estimator= SVC(C=best_params['C'],kernel=best_params['kernel'],gamma=best_params['gamma'],degree=best_params['degree'],probability=best_params['probability'],class_weight=class_weight)\n",
    "        elif model=='rf':\n",
    "            estimator= RandomForestClassifier(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth'],min_samples_split=best_params['min_samples_split'],min_samples_leaf=best_params['min_samples_leaf'],bootstrap=best_params['bootstrap'],class_weight='balanced')\n",
    "        elif model=='cnb':\n",
    "            estimator= ComplementNB(alpha=best_params['alpha'],fit_prior=best_params['fit_prior'],norm=best_params['norm'])\n",
    "        elif model=='xgb':\n",
    "            ratio_w=y_train_1.value_counts()[0]/y_train_1.value_counts()[1]\n",
    "            estimator=XGBClassifier(min_child_weight=best_params['min_child_weight'],gamma=best_params['gamma'],subsample=best_params['subsample'],colsample_bytree=best_params['colsample_bytree'],max_depth=best_params['max_depth'], scale_pos_weight=ratio_w)\n",
    "         \n",
    "        #check whether sample weights have to be applied and fit estimator\n",
    "        if sampleweightyes:\n",
    "            sample_weight = compute_sample_weight(class_weight='balanced', y=y_train_1)\n",
    "            estimator.fit(X_train_1, y_train_1,sample_weight=sample_weight)\n",
    "        else:\n",
    "            estimator.fit(X_train_1,y_train_1)\n",
    "        \n",
    "        #predict labels\n",
    "        y_pred=estimator.predict(X_val)\n",
    "        \n",
    "        #predict probabilities\n",
    "        y_probas=estimator.predict_proba(X_val)\n",
    "        \n",
    "        #calculate f2 score before thresholding\n",
    "        f2_score_before_thresholding=fbeta_score(y_val,y_pred,beta=2)\n",
    "        \n",
    "        #calculate optimal threshold\n",
    "        p, r, thresholds = precision_recall_curve(y_val, y_probas[:,1])\n",
    "        f2_list=[]\n",
    "        for i in range(0,len(p)):\n",
    "            f2_list.append((5*p[i]*r[i])/((4*p[i])+r[i]))\n",
    "        t=thresholds[f2_list.index(max(f2_list))]\n",
    "        \n",
    "        #calculate new labels and optimal f2 score\n",
    "        new_labels=adjusted_classes(y_probas[:,1],t)\n",
    "        f2_score=fbeta_score(y_val,new_labels,beta=2)\n",
    "        return t\n",
    "\n",
    "    def test_model(model,best_params,X_train,y_train,X_test,y_test,t,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):\n",
    "        #test model for final validation\n",
    "        \n",
    "        \n",
    "        X_train,X_test=X_train.astype(str),X_test.astype(str) \n",
    "        \n",
    "        #extract ngram representation\n",
    "        ngram_range = extract_ngram(text_representation)\n",
    "        \n",
    "        #perform vectorization of text data\n",
    "        if feature_weighting =='TF-IDF':\n",
    "            X_train,X_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),True)\n",
    "        elif feature_weighting =='TF':\n",
    "            X_train,X_test=perform_tf_idf(X_train,X_test,ngram_range,wordchar.lower(),False)\n",
    "        elif feature_weighting=='BINARY':\n",
    "            X_train,X_test=perform_countvectorizer(X_train,X_test,ngram_range,wordchar.lower())\n",
    "        elif feature_weighting=='INFO-GAIN':\n",
    "            X_train,X_test=perform_info_gain_df(X_train,X_test,y_train,ngram_range,wordchar.lower())   \n",
    "        \n",
    "        #check whether SMOTE has to be performed\n",
    "        if smoteennyes:\n",
    "            X_train, y_train = SMOTEENN().fit_resample(X_train, y_train)\n",
    "        \n",
    "        #check whether undersampling has to be performed\n",
    "        if underyes:\n",
    "            X_train,y_train =RandomUnderSampler(sampling_strategy=1).fit_resample(X_train,y_train)\n",
    "        \n",
    "        #check whether classweights have to be applied\n",
    "        if classweightyes:\n",
    "            class_weight='balanced'\n",
    "        else:\n",
    "            class_weight=None\n",
    "       \n",
    "        #check which estimator will be used and use optimal parameters\n",
    "        if model=='svm':\n",
    "            estimator= SVC(C=best_params['C'],kernel=best_params['kernel'],gamma=best_params['gamma'],degree=best_params['degree'],probability=best_params['probability'],class_weight=class_weight)\n",
    "        elif model=='rf':\n",
    "            estimator= RandomForestClassifier(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth'],min_samples_split=best_params['min_samples_split'],min_samples_leaf=best_params['min_samples_leaf'],bootstrap=best_params['bootstrap'],class_weight='balanced')\n",
    "        elif model=='cnb':\n",
    "            estimator= ComplementNB(alpha=best_params['alpha'],fit_prior=best_params['fit_prior'],norm=best_params['norm'])\n",
    "        elif model=='xgb':\n",
    "            ratio_w=y_train.value_counts()[0]/y_train.value_counts()[1]\n",
    "            estimator=XGBClassifier(min_child_weight=best_params['min_child_weight'],gamma=best_params['gamma'],subsample=best_params['subsample'],colsample_bytree=best_params['colsample_bytree'],max_depth=best_params['max_depth'], scale_pos_weight=ratio_w)\n",
    "         \n",
    "        #check whether sample weights have to be applied and fit estimator\n",
    "        if sampleweightyes:\n",
    "            sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "            estimator.fit(X_train, y_train,sample_weight=sample_weight)\n",
    "        else:\n",
    "            estimator.fit(X_train,y_train)\n",
    "        \n",
    "        #predict labels\n",
    "        y_pred=estimator.predict(X_test)\n",
    "        \n",
    "        #predict probabilities\n",
    "        y_probas=estimator.predict_proba(X_test)\n",
    "        \n",
    "        #calculate f2 score before thresholding\n",
    "        f2_score_without_thresholding=fbeta_score(y_test,y_pred,beta=2)\n",
    "        \n",
    "        #calculate new labels and optimal f2 score\n",
    "        new_labels=adjusted_classes(y_probas[:,1],t)\n",
    "        f2_score=fbeta_score(y_test,new_labels,beta=2)\n",
    "        \n",
    "        #calculate roc-auc score,precision,fpr,fnr and recall\n",
    "        roc_score=roc_auc_score(y_test,y_probas[:,1])\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test,new_labels).ravel()\n",
    "        precision=tp/(tp+fp)\n",
    "        fpr = fp/(fp+tn)\n",
    "        fnr=fn/(fn+tp)\n",
    "        recall=tp/(tp+fn)\n",
    "        \n",
    "        return f2_score, f2_score_without_thresholding,roc_score,precision,fpr,fnr,recall\n",
    "    \n",
    "    #fit and validate model and use optimal threshold of training data on test data\n",
    "    t=fit_and_val_model(model,best_params,X_train_1,y_train_1,X_val,y_val,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting)\n",
    "    return test_model(model,best_params,X_train,y_train,X_test,y_test,t,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting)\n",
    "\n",
    "\n",
    "\n",
    "def testing_statistics(data,model,best_params,iterations,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting):  #oneven aantal!!!\n",
    "    #this function validates the model x iterations and saves all scores\n",
    "    \n",
    "    f2_thresh_scores=[]\n",
    "    f2_no_thresh_scores=[]\n",
    "    roc_scores=[]\n",
    "    precision_scores=[]\n",
    "    fpr_scores=[]\n",
    "    fnr_scores=[]\n",
    "    recall_scores=[]\n",
    "    \n",
    "    for iter in range(0,iterations):\n",
    "        f2_thresh,f2_no_thresh,roc_score,precision,fpr,fnr,recall=do_testing(data,model,best_params,smoteennyes,underyes,classweightyes,sampleweightyes,text_representation,wordchar,feature_weighting)\n",
    "        f2_thresh_scores.append(f2_thresh)\n",
    "        f2_no_thresh_scores.append(f2_no_thresh)\n",
    "        roc_scores.append(roc_score)\n",
    "        precision_scores.append(precision)\n",
    "        fpr_scores.append(fpr)\n",
    "        fnr_scores.append(fnr)\n",
    "        recall_scores.append(recall)\n",
    "    return f2_thresh_scores, f2_no_thresh_scores,roc_scores,precision_scores,fpr_scores,fnr_scores,recall_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
