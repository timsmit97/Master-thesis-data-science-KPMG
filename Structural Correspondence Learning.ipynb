{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libtlda.scl import StructuralCorrespondenceClassifier\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, roc_auc_score,matthews_corrcoef, precision_recall_curve, fbeta_score, make_scorer,mean_absolute_error,accuracy_score,jaccard_score\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,RandomizedSearchCV,GridSearchCV,cross_val_predict\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from scipy.sparse import linalg\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from os.path import basename\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "class StructuralCorrespondenceClassifier(object):\n",
    "    \"\"\"\n",
    "    Class of classifiers based on structural correspondence learning.\n",
    "    Methods consist of a way to augment features, and a Huber loss function\n",
    "    plus gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss='logistic', l2=1.0, num_pivots=1,\n",
    "                 num_components=1,MI_selection='freq',freq_threshold=100):\n",
    "        \"\"\"\n",
    "        Select a particular type of importance-weighted classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss : str\n",
    "            loss function for weighted classifier, options: 'logistic',\n",
    "                'quadratic', 'hinge' (def: 'logistic')\n",
    "        l2 : float\n",
    "            l2-regularization parameter value (def:0.01)\n",
    "        num_pivots : int\n",
    "            number of pivot features to use (def: 1)\n",
    "        num_components : int\n",
    "            number of components to use after extracting pivot features\n",
    "            (def: 1)\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.loss = loss\n",
    "        self.l2 = l2\n",
    "        self.num_pivots = num_pivots\n",
    "        self.num_components = num_components\n",
    "        self.MI_selection=MI_selection\n",
    "        self.freq_threshold=freq_threshold\n",
    "        self.source_dataset_dict={}\n",
    "        self.target_dataset_dict={}\n",
    "        \n",
    "        # Initialize untrained classifiers based on choice of loss function\n",
    "        if self.loss == 'logistic':\n",
    "            # Logistic regression model\n",
    "            self.clf = LogisticRegression()\n",
    "        elif self.loss == 'rf':\n",
    "            # Least-squares model\n",
    "            self.clf = RandomForestClassifier()\n",
    "        elif self.loss == 'svm':\n",
    "            # Linear support vector machine\n",
    "            self.clf = SVC(probability=True)\n",
    "        else:\n",
    "            # Other loss functions are not implemented\n",
    "            raise NotImplementedError('Loss not implemented yet.')\n",
    "\n",
    "        # Whether model has been trained\n",
    "        self.is_trained = False\n",
    "\n",
    "        # Maintain pivot component matrix\n",
    "        self.C = 0\n",
    "\n",
    "        # Dimensionality of training data\n",
    "        self.train_data_dim = ''\n",
    "        \n",
    "        # Parameter grids of algorithms\n",
    "        self.svm_grid = {'C': [.01,.1,.4,.6,1,1.5],\n",
    "              'kernel': ['rbf','poly','linear'],\n",
    "              'gamma':  [.01, .1, 1],\n",
    "              'degree': [1, 2, 3, 4, 5],\n",
    "              'probability': [True],\n",
    "              'class_weight':['balanced']\n",
    "             }\n",
    "\n",
    "        self.rf_grid={'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "                       'max_features': ['auto', 'sqrt'],\n",
    "                       'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "                       'min_samples_split': [2, 5, 10],\n",
    "                       'min_samples_leaf': [1, 2, 4],\n",
    "                       'bootstrap': [True, False],\n",
    "                        'class_weight':['balanced']}\n",
    "\n",
    "    def augment_features_exp(self, X,y, Z,y_z ,l2=0.0):\n",
    "        \n",
    "        \"\"\"\n",
    "        Find a set of pivot features, train predictors and extract bases.\n",
    "        Parameters\n",
    "        X : array\n",
    "            source data array (N samples by D features)\n",
    "        Z : array\n",
    "            target data array (M samples by D features)\n",
    "        l2 : float\n",
    "            regularization parameter value (def: 0.0)\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        # create word frequency dictionaries\n",
    "        self.source_dataset_dict=self.create_dataset_dict(X)\n",
    "        self.target_dataset_dict=self.create_dataset_dict(Z)\n",
    "        \n",
    "        # Data shapes\n",
    "        N, DX = X.shape\n",
    "        M, DZ = Z.shape\n",
    "   \n",
    "        \n",
    "\n",
    "        # merge the datasets\n",
    "        columns_conc=pd.concat([X, Z]).columns\n",
    "        XZ=pd.concat([X, Z]).fillna(0).to_numpy()\n",
    "    \n",
    "        # shape of dataset\n",
    "        K, DXZ = XZ.shape\n",
    "\n",
    "        #select pivots on Mutual Information (MI) in source domain\n",
    "        if self.MI_selection=='mi':\n",
    "            ix=self.select_on_MI(X, y, self.num_pivots)\n",
    "            \n",
    "        #select pivots on Mutual Information (MI) in source domain and target domain\n",
    "        elif self.MI_selection=='mimi':\n",
    "            ix=self.select_on_MIMI(X,y,Z, y_z, self.num_pivots)\n",
    "        \n",
    "        #select pivots on frequency\n",
    "        elif self.MI_selection=='freq':\n",
    "            # Sort indices based on frequency of features (assumes BoW encoding)\n",
    "            ix = np.argsort(np.sum(XZ, axis=0))\n",
    "            # Keep most frequent features\n",
    "            ix = ix[::-1][:self.num_pivots]\n",
    "            \n",
    "        # Slice out pivot features and relabel them as present(=1)/absent(=0)\n",
    "        pivot = (XZ[:, ix] > 0).astype('float') \n",
    "       \n",
    "        # Solve prediction tasks with a Huber loss function\n",
    "        P = np.zeros((DXZ, self.num_pivots))  \n",
    "\n",
    "        # Loop over pivot features\n",
    "        for l in range(self.num_pivots):\n",
    "\n",
    "            # Setup loss function for single pivot\n",
    "            def L(theta): return self.Huber_loss(theta, XZ, pivot[:, l])\n",
    "\n",
    "            # Setup gradient function for single pivot\n",
    "            def J(theta): return self.Huber_grad(theta, XZ, pivot[:, l])\n",
    "\n",
    "            # Make pivot predictor with a Huber loss function\n",
    "            results = minimize(L, np.random.randn(DXZ, 1), jac=J, method='BFGS',   #DX hier veranderd naar DXZ\n",
    "                               options={'gtol': 1e-6, 'disp': True})\n",
    "\n",
    "            # Store optimal parameters\n",
    "            P[:, l] = results.x\n",
    "       \n",
    "        \n",
    "        \n",
    "        # Compute covariance matrix of predictors  \n",
    "        SP = np.cov(P)  \n",
    "       \n",
    "        # Add regularization to ensure positive-definiteness\n",
    "        test=l2*np.eye(DXZ)\n",
    "        \n",
    "        SP += test       \n",
    "        \n",
    "        # Eigenvalue decomposition of pivot predictor matrix\n",
    "        V, C = np.linalg.eigh(SP)\n",
    "    \n",
    "        # Reduce number of components\n",
    "        C = C[:, :self.num_components]\n",
    "    \n",
    "        #Dimensionality correction\n",
    "        intersect_cols=[]\n",
    "        for column in set(X.columns).intersection(Z.columns):\n",
    "            intersect_cols.append(X.columns.get_loc(column))\n",
    "        C_z=C[intersect_cols,:]\n",
    "        rest_cols=len(Z.columns)-len(intersect_cols)\n",
    "        C_zz=C[len(X.columns):(len(X.columns)+rest_cols+1),:]\n",
    "        C_z=np.concatenate([C_z,C_zz],axis=0)\n",
    "            \n",
    "        # Augment features\n",
    "        Xa=pd.DataFrame(data=np.concatenate((np.dot(X, C[0:len(X.columns),:]), X), axis=1))\n",
    "        Za=pd.DataFrame(data=np.concatenate((np.dot(Z, C_z), Z), axis=1))\n",
    " \n",
    "        return Xa, Za, C  \n",
    "    \n",
    "    \n",
    "    def Huber_loss(self, theta, X, y, l2=0.0):\n",
    "        \"\"\"\n",
    "        Huber loss function.\n",
    "        Reference: Ando & Zhang (2005a). A framework for learning predictive\n",
    "        structures from multiple tasks and unlabeled data. JMLR.\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : array\n",
    "            classifier parameters (D features by 1)\n",
    "        X : array\n",
    "            data (N samples by D features)\n",
    "        y : array\n",
    "            label vector (N samples by 1)\n",
    "        l2 : float\n",
    "            l2-regularization parameter (def= 0.0)\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            Objective function value.\n",
    "        \"\"\"\n",
    "        # Precompute terms\n",
    "        Xy = (X.T*y.T).T\n",
    "        Xyt = np.dot(Xy, theta)\n",
    "\n",
    "        # Indices of discontinuity\n",
    "        ix = (Xyt >= -1)\n",
    "\n",
    "        # Loss function\n",
    "        return np.sum(np.clip(1 - Xyt[ix], 0, None)**2, axis=0) \\\n",
    "            + np.sum(-4*Xyt[~ix], axis=0) + l2*np.sum(theta**2, axis=0)\n",
    "\n",
    "    def Huber_grad(self, theta, X, y, l2=0.0):\n",
    "        \"\"\"\n",
    "        Huber gradient computation.\n",
    "        Reference: Ando & Zhang (2005a). A framework for learning predictive\n",
    "        structures from multiple tasks and unlabeled data. JMLR.\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : array\n",
    "            classifier parameters (D features by 1)\n",
    "        X : array\n",
    "            data (N samples by D features)\n",
    "        y : array\n",
    "            label vector (N samples by 1)\n",
    "        l2 : float\n",
    "            l2-regularization parameter (def= 0.0)\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            Gradient with respect to classifier parameters\n",
    "        \"\"\"\n",
    "        # Precompute terms\n",
    "        Xy = (X.T*y.T).T\n",
    "        Xyt = np.dot(Xy, theta)\n",
    "\n",
    "        # Indices of discontinuity\n",
    "        ix = (Xyt >= -1)\n",
    "\n",
    "        # Gradient\n",
    "        return np.sum(2*np.clip(1-Xyt[ix], 0, None).T * -Xy[ix, :].T,\n",
    "                      axis=1).T + np.sum(-4*Xy[~ix, :], axis=0) + 2*l2*theta\n",
    "    \n",
    "    def fit_nieuw(self, X, y, Z,y_t):\n",
    "        \"\"\"\n",
    "        Fit/train an structural correpondence classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array\n",
    "            source data (N samples by D features)\n",
    "        y : array\n",
    "            source labels (N samples by 1)\n",
    "        Z : array\n",
    "            target data (M samples by D features)\n",
    "        y_t: array\n",
    "            source labels (N samples by 1)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        target_augmented: array\n",
    "                          augmented target data (N samples by D + self.num_components features)\n",
    "        random_search:    object\n",
    "                          fitted RandomizedSearchCV object with optimal hyperparameters\n",
    "        t                 double\n",
    "                          optimal decision threshold based on 10% target data\n",
    "        y_train           array\n",
    "                          source labels (K samples by 1)\n",
    "        precision         double\n",
    "                          precision score on test data\n",
    "        recall            double\n",
    "                          recall score on test data\n",
    "                          \n",
    "        \"\"\"\n",
    "        \n",
    "        # Data shapes\n",
    "        N, DX = X.shape\n",
    "        M, DZ = Z.shape\n",
    "\n",
    "\n",
    "        # Augment features\n",
    "        X, target_augmented, self.C = self.augment_features_exp(X,y, Z,y_t, l2=self.l2)\n",
    "        \n",
    "        \n",
    "        # Initialize parameter grid based on algorithm\n",
    "        if self.loss=='rf':\n",
    "            grid=self.rf_grid\n",
    "        elif self.loss=='svm':\n",
    "            grid=self.svm_grid\n",
    "            \n",
    "        \n",
    "        # Create scoring function to evaluate performance on F2-score\n",
    "        ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "        \n",
    "        # Initialize random search object with f2 scoring\n",
    "        random_search = RandomizedSearchCV(estimator=self.clf,\n",
    "                                   param_distributions=grid,\n",
    "                                   n_iter=12,\n",
    "                                   scoring=ftwo_scorer,\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   refit=True)\n",
    "       \n",
    "        \n",
    "       \n",
    "        # Mark classifier as trained\n",
    "        self.is_trained = True\n",
    "\n",
    "        # Store training data dimensionality\n",
    "        self.train_data_dim = DX + self.num_components\n",
    "        \n",
    "        #split train/test to enable thresholding on target domain\n",
    "        target_augmented, target_augmented_test, y_train, y_test=self.split_train_test(target_augmented,y_t)\n",
    "        sample_weight = compute_sample_weight(class_weight='balanced', y=y)\n",
    "        \n",
    "        # Run random search algorithm\n",
    "        random_search.fit(X,y,sample_weight=sample_weight)\n",
    "        \n",
    "        # Find optimal decision threshold\n",
    "        t,precision,recall=self.thresholden(random_search,y_test,target_augmented_test)\n",
    "        \n",
    "        return target_augmented,random_search,t,y_train,precision,recall\n",
    "        \n",
    "    \n",
    "    def split_train_test(self,X_train,y_train):\n",
    "        \"\"\"\n",
    "        This function splits the data in train and test set\n",
    "        \"\"\"\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train,  \n",
    "                                                       y_train, \n",
    "                                                        test_size=0.15,stratify=y_train)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    def adjusted_classes(self,y_scores, t):\n",
    "        \"\"\"\n",
    "        This function adjusts class predictions based on the prediction threshold (t).\n",
    "        \"\"\"\n",
    "        return [1 if y >= t else 0 for y in y_scores]\n",
    "    \n",
    "    def predict(self, Z):\n",
    "        \"\"\"\n",
    "        Make predictions on new dataset.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : array\n",
    "            new data set (M samples by D features)\n",
    "        Returns\n",
    "        -------\n",
    "        preds : array\n",
    "            label predictions (M samples by 1)\n",
    "        \"\"\"\n",
    "        # Data shape\n",
    "        M, D = Z.shape\n",
    "\n",
    "        # If classifier is trained, check for same dimensionality\n",
    "        if self.is_trained:\n",
    "            if not self.train_data_dim == D:\n",
    "                raise ValueError('''Test data is of different dimensionality\n",
    "                                 than training data.''')\n",
    "\n",
    "        # Check for augmentation\n",
    "        if not self.train_data_dim == D:\n",
    "            Z = np.concatenate((np.dot(Z, self.C), Z), axis=1)\n",
    "\n",
    "        # Call scikit's predict function\n",
    "        preds = self.clf.predict(Z)\n",
    "        \n",
    "        # For quadratic loss function, correct predictions\n",
    "        if self.loss == 'quadratic':\n",
    "            preds = (np.sign(preds)+1)/2.\n",
    "\n",
    "        # Return predictions array\n",
    "        return preds\n",
    "    \n",
    "    def predict_proba(self, Z):\n",
    "        \"\"\"\n",
    "        Makes probability predictions on new dataset.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : array\n",
    "            new data set (M samples by D features)\n",
    "        Returns\n",
    "        -------\n",
    "        preds : array\n",
    "            label predictions (M samples by 1)\n",
    "        \"\"\"\n",
    "        # Data shape\n",
    "        M, D = Z.shape\n",
    "\n",
    "        # If classifier is trained, check for same dimensionality\n",
    "        if self.is_trained:\n",
    "            if not self.train_data_dim == D:\n",
    "                raise ValueError('''Test data is of different dimensionality\n",
    "                                 than training data.''')\n",
    "\n",
    "        # Check for augmentation\n",
    "        if not self.train_data_dim == D:\n",
    "            Z = np.concatenate((np.dot(Z, self.C), Z), axis=1)\n",
    "\n",
    "        # Call scikit's predict_proba function\n",
    "        probas = self.clf.predict_proba(Z)\n",
    "        \n",
    "      \n",
    "        # Return predictions array\n",
    "        return probas\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"Get classifier parameters.\"\"\"\n",
    "        return self.clf.get_params()\n",
    "    \n",
    "    def create_dataset_dict(self,dataset):\n",
    "        \"\"\"\n",
    "        This function creates a dictionary with the word frequencies\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : array\n",
    "                  data set (M samples by D features)\n",
    "        \"\"\"\n",
    "        dict_dataset={}\n",
    "        for feature in dataset.columns:\n",
    "            dict_dataset[feature]=dataset[feature].sum()\n",
    "        return dict_dataset\n",
    "    \n",
    "    def thresholden(self,random_search,y_val,X_val):\n",
    "        \"\"\"\n",
    "        This function finds the optimal decision threshold t for the classifier.\n",
    "        \"\"\"\n",
    "        # generate probabilities\n",
    "        y_probas=random_search.predict_proba(X_val)\n",
    "        \n",
    "        # generate scores based on different thresholds\n",
    "        p, r, thresholds = precision_recall_curve(y_val, y_probas[:,1])\n",
    "        f2_list=[]\n",
    "        for i in range(0,len(p)):\n",
    "            f2_list.append((5*p[i]*r[i])/((4*p[i])+r[i]))\n",
    "        \n",
    "        # find maximum f2 score generated by all possible decision thresholds\n",
    "        t=thresholds[f2_list.index(max(f2_list))]\n",
    "        return t,p,r\n",
    "\n",
    "    def is_trained(self):\n",
    "        \"\"\"Check whether classifier is trained.\"\"\"\n",
    "        return self.is_trained\n",
    "    \n",
    "    def select_on_MI(self,X,y,num_pivots):\n",
    "         \"\"\"\n",
    "        This function selects the features with the highest MI and which frequencies are above a threshold\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array\n",
    "            source data (N samples by D features)\n",
    "        y : array\n",
    "            source labels (N samples by 1)\n",
    "        num_pivots : int\n",
    "                     number of desired pivot features\n",
    "        \"\"\"\n",
    "            \n",
    "        # select possible pivot feature candidates on frequency\n",
    "        features_freq=[feature for feature in X.columns if self.source_dataset_dict[feature]>self.freq_threshold and feature in self.target_dataset_dict and self.target_dataset_dict[feature]>self.freq_threshold]\n",
    "        \n",
    "        # sort the candidates on their mutual information\n",
    "        sorted_MIs=np.argsort(mutual_info_classif(X[features_freq],y , discrete_features=True))\n",
    "        \n",
    "        return sorted_MIs[-num_pivots:]\n",
    "    \n",
    "    def select_on_MIMI(self,X,y,Z,y_t,num_pivots):   #functie krijgt 10% target data mee\n",
    "        \"\"\"\"\"\n",
    "        This function selects the features with the highest MI and which frequencies are above a threshold\n",
    "         ----------\n",
    "         Parameters\n",
    "         ----------\n",
    "        X : array\n",
    "            source data (N samples by D features)\n",
    "        y : array\n",
    "            source labels (N samples by 1)\n",
    "        Z : array\n",
    "            target data (M samples by D features)\n",
    "        y_t: array\n",
    "            source labels (N samples by 1)\n",
    "        num_pivots : int\n",
    "                     number of desired pivot features\n",
    "        \"\"\"\"\"\n",
    "        \n",
    "        # select possible pivot feature candidates on frequency\n",
    "        features_freq=[feature for feature in X.columns if self.source_dataset_dict[feature]>self.freq_threshold and feature in self.target_dataset_dict and self.target_dataset_dict[feature]>self.freq_threshold]\n",
    "        \n",
    "        # select 10% target domain data for MI selection \n",
    "        Z_rest, Z_MI, Z_y_rest, Z_y_MI=self.split_train_test(Z,y_t)\n",
    "        \n",
    "        # MI scores in source domain\n",
    "        MI_scores_Z=mutual_info_classif(Z_MI[features_freq],Z_y_MI , discrete_features=True)\n",
    "        \n",
    "        # MI scores in target domain\n",
    "        MI_scores_X=mutual_info_classif(X[features_freq],y , discrete_features=True)\n",
    "        \n",
    "        # Combine MI scores\n",
    "        MI_scores_comb=MI_scores_Z+MI_scores_X\n",
    "        \n",
    "        # Sort combined MI scores\n",
    "        sorted_MIs=np.argsort(MI_scores_comb)\n",
    "        \n",
    "        return sorted_MIs[-num_pivots:]\n",
    "        \n",
    "    \n",
    "\n",
    "def undersample(features_train,labels_train):\n",
    "    \"\"\"\"\"\n",
    "        This function undersamples the majority class\n",
    "         ----------\n",
    "         Parameters\n",
    "         ----------\n",
    "        features_train : array\n",
    "            source data (N samples by D features)\n",
    "        labels_train : array\n",
    "            source labels (N samples by 1)\n",
    "        \"\"\"\"\"\n",
    "\n",
    "    features_train, labels_train =RandomUnderSampler(sampling_strategy=1).fit_resample(features_train,labels_train)\n",
    "    \n",
    "    return features_train,labels_train\n",
    "\n",
    "\n",
    "\n",
    "def oversample(features_train,labels_train):\n",
    "    \"\"\"\"\"\n",
    "        This function undersamples the minority class\n",
    "         ----------\n",
    "         Parameters\n",
    "         ----------\n",
    "        features_train : array\n",
    "            source data (N samples by D features)\n",
    "        labels_train : array\n",
    "            source labels (N samples by 1)\n",
    "        \"\"\"\"\"\n",
    "    features_train, labels_train =RandomOverSampler(sampling_strategy=0.5).fit_resample(features_train,labels_train)\n",
    "    return features_train,labels_train\n",
    "\n",
    "def create_balanced_sets(dfsource,source_labels,dftarget,target_labels):\n",
    "     \"\"\"\"\"\n",
    "        This function creates balanced datasets for both domains\n",
    "         ----------\n",
    "         Parameters\n",
    "         ----------\n",
    "        dfsource : array\n",
    "            source data (N samples by D features)\n",
    "        source_labels : array\n",
    "            source labels (N samples by 1)\n",
    "        dftarget : array\n",
    "            source data (M samples by D features)\n",
    "        target_labels : array\n",
    "            source labels (M samples by 1)\n",
    "        \"\"\"\"\"\n",
    "    \n",
    "    dfsource,source_labels=undersample(dfsource,source_labels)\n",
    "    dftarget,target_labels=undersample(dftarget,target_labels)\n",
    "    return dfsource,source_labels,dftarget,target_labels\n",
    "\n",
    "def init_lists():\n",
    "    \"\"\"\"\"\n",
    "        This function initializes empty scoring lists\n",
    "    \"\"\"\"\n",
    "    lists=[[] for i in range(8)]\n",
    "    return lists[0],lists[1],lists[2],lists[3],lists[4],lists[5],lists[6],lists[7]\n",
    "\n",
    "def update_lists(roc_auc_list,roc_auc,f2_score_before_thresholding_list,f2_score_before_thresholding,fpr_list,fpr,fnr_list,fnr,f2_score_list_after,f2_at,fpr_list_after,fpr_at,fnr_list_after,fnr_at,acc_list,acc):\n",
    "    \"\"\"\"\"\n",
    "        This function updates the lists with individual metrics\n",
    "    \"\"\"\"\n",
    "    roc_auc_list.append(roc_auc)\n",
    "    f2_score_before_thresholding_list.append(f2_score_before_thresholding)\n",
    "    fpr_list.append(fpr)\n",
    "    fnr_list.append(fnr)\n",
    "    f2_score_list_after.append(f2_at)\n",
    "    fpr_list_after.append(fpr_at)\n",
    "    fnr_list_after.append(fnr_at)\n",
    "    acc_list.append(acc)\n",
    "    return roc_auc_list,f2_score_before_thresholding_list,fpr_list,fnr_list,f2_score_list_after,fpr_list_after,fnr_list_after,acc_list\n",
    "\n",
    "def calc_and_print_scores(y_test,y_pred):\n",
    "    \"\"\"\"\"\n",
    "        This function calculates the important metrics from the predictions and ground truth\n",
    "         ----------\n",
    "         Parameters\n",
    "         ----------\n",
    "        y_test : array\n",
    "            source labels (N samples by 1)\n",
    "        y_pred : array\n",
    "            predictions (N samples by 1)\n",
    "    \"\"\"\"\n",
    "    f2=fbeta_score(y_test,y_pred,beta=2)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n",
    "    fnr=fn/(fn+tp)\n",
    "    fpr=fp/(fp+tn)\n",
    "    \n",
    "    return f2,fnr,fpr\n",
    "\n",
    "def perform_countvect(X_train,X_test,ngram_range,analyzer,num_features):\n",
    "    \"\"\"\"\"\n",
    "    This function calculates the important metrics from the predictions and ground truth\n",
    "     ----------\n",
    "     Parameters\n",
    "     ----------\n",
    "    X_train : array\n",
    "              source features (N samples by D features)\n",
    "    X_test : array\n",
    "             source features (M samples by D features)\n",
    "    ngram_range : tuple  (min_n, max_n)\n",
    "                  The lower and upper boundary of the range of n-values for \n",
    "                  different word n-grams or char n-grams to be extracted.\n",
    "    analyzer : string {‘word’, ‘char’, ‘char_wb’} \n",
    "               Whether the feature should be made of word n-gram or character n-grams.\n",
    "    num_features : int \n",
    "                   K features to select by the vectorizer\n",
    "        \n",
    "    \"\"\"\"\n",
    "    X_train,X_test=X_train.astype(str),X_test.astype(str)\n",
    "    \n",
    "    min_df = 10\n",
    "    max_df = 1.\n",
    "    max_features = num_features\n",
    "    countvect = CountVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,analyzer=analyzer,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                               binary=True)\n",
    "    features_train = countvect.fit_transform(X_train).toarray()\n",
    "    features_train = pd.DataFrame(features_train, columns=countvect.get_feature_names())\n",
    "    features_test = countvect.transform(X_test).toarray()\n",
    "    features_test = pd.DataFrame(features_test, columns=countvect.get_feature_names())\n",
    "    return features_train,features_test\n",
    "\n",
    "def extract_ngram(text_representation):\n",
    "      \"\"\"\"\"\n",
    "    This function sets the tuple correct for the text representation\n",
    "     ----------\n",
    "     Parameters\n",
    "     ----------\n",
    "    text_representation : string {'UNIGRAM','BIGRAM','TRIGRAM'}\n",
    "              source features (N samples by D features)\n",
    "    \n",
    "    \"\"\"\"\n",
    "    if text_representation=='UNIGRAM':\n",
    "        ngram_range=(1,1)\n",
    "    elif text_representation=='BIGRAM':\n",
    "        ngram_range=(1,2)\n",
    "    elif text_representation=='TRIGRAM':\n",
    "        ngram_range=(1,3)\n",
    "    return ngram_range\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
